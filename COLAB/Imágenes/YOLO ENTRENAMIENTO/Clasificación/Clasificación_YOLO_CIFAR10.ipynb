{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":873},"id":"THyMZyH4SZ8a","executionInfo":{"status":"ok","timestamp":1728416665947,"user_tz":360,"elapsed":71405,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"1a181884-1852-4a8e-a01d-45d5ea323887"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-a5f0e66f-d2cb-409f-a3fd-6cde9d269faa\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-a5f0e66f-d2cb-409f-a3fd-6cde9d269faa\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n","Downloading cifar-10.zip to /content\n","100% 713M/715M [00:09<00:00, 92.6MB/s]\n","100% 715M/715M [00:09<00:00, 76.0MB/s]\n","Archive:  cifar-10.zip\n","  inflating: ./cifar-10/sampleSubmission.csv  \n","  inflating: ./cifar-10/test.7z      \n","  inflating: ./cifar-10/train.7z     \n","  inflating: ./cifar-10/trainLabels.csv  \n","Collecting py7zr\n","  Downloading py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\n","Collecting texttable (from py7zr)\n","  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n","Collecting pycryptodomex>=3.16.0 (from py7zr)\n","  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Collecting pyzstd>=0.15.9 (from py7zr)\n","  Downloading pyzstd-0.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n","Collecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n","  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n","Collecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n","  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n","Collecting multivolumefile>=0.2.3 (from py7zr)\n","  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n","Collecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n","  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n","Collecting brotli>=1.1.0 (from py7zr)\n","  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from py7zr) (5.9.5)\n","Downloading py7zr-0.22.0-py3-none-any.whl (67 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n","Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyzstd-0.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.8/413.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n","Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\n","Successfully installed brotli-1.1.0 inflate64-1.0.0 multivolumefile-0.2.3 py7zr-0.22.0 pybcj-1.0.2 pycryptodomex-3.21.0 pyppmd-1.1.0 pyzstd-0.16.1 texttable-1.7.0\n"]}],"source":["# 1. Instalar y cargar Kaggle, descomprimir el dataset\n","!pip install -q kaggle\n","from google.colab import files\n","\n","# Subir kaggle.json con tus credenciales\n","uploaded = files.upload()\n","\n","# Crear el directorio .kaggle y mover kaggle.json\n","!mkdir -p ~/.kaggle\n","!cp \"{list(uploaded.keys())[0]}\" ~/.kaggle/kaggle.json\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","# Descargar y descomprimir el dataset CIFAR-10\n","!kaggle competitions download -c cifar-10\n","!unzip cifar-10.zip -d ./cifar-10\n","\n","# Instalar librería para descomprimir .7z\n","!pip install py7zr\n","import py7zr\n","\n","# Descomprimir las imágenes de entrenamiento\n","with py7zr.SevenZipFile('/content/cifar-10/train.7z', mode='r') as z:\n","    z.extractall(path='/content/cifar-10/train')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"veITmubYekQ_","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1728416666557,"user_tz":360,"elapsed":618,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"cedf0843-3b29-4676-c90f-5b8fe4df23c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Imagen: 38573.png, Tamaño: 32x32\n","Imagen: 15381.png, Tamaño: 32x32\n","Imagen: 27481.png, Tamaño: 32x32\n","Imagen: 278.png, Tamaño: 32x32\n","Imagen: 29573.png, Tamaño: 32x32\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 5 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# 2. Hacer un muestreo del dataset para verificar su contenido y tamaño\n","import os\n","import random\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# Directorio de las imágenes\n","train_dir = '/content/cifar-10/train/train'\n","\n","# Obtener una lista de archivos de imagen\n","image_files = [f for f in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, f))]\n","\n","# Seleccionar 5 imágenes aleatoriamente\n","random_images = random.sample(image_files, 5)\n","\n","# Mostrar las imágenes y sus dimensiones\n","plt.figure(figsize=(10,5))\n","for i, image_file in enumerate(random_images):\n","    img = Image.open(os.path.join(train_dir, image_file))\n","    width, height = img.size\n","    print(f\"Imagen: {image_file}, Tamaño: {width}x{height}\")\n","    plt.subplot(1, 5, i + 1)\n","    plt.imshow(img)\n","    plt.axis('off')\n","    plt.title(image_file)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WhxWI75NgwaV"},"source":["# **Clases de CIFAR-10**\n","airplane\n","automobile\n","bird\n","cat\n","deer\n","dog\n","frog\n","horse\n","ship\n","truck\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"g7mJ78XCgu3q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728416667002,"user_tz":360,"elapsed":453,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"3567e886-d935-4d3e-93a2-cf68772dcaa7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 50000\n"]}],"source":["# 3. Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/cifar-10/train/train'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"d2tVT9V1iFnl","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1728416667577,"user_tz":360,"elapsed":580,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"880a20ce-e2f1-4969-e10b-cdd8199f6a76"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id       label\n","0   1        frog\n","1   2       truck\n","2   3       truck\n","3   4        deer\n","4   5  automobile"],"text/html":["\n","  <div id=\"df-fff6f95c-6c50-4564-8584-cb14283cfc81\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>frog</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>truck</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>truck</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>deer</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>automobile</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fff6f95c-6c50-4564-8584-cb14283cfc81')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-fff6f95c-6c50-4564-8584-cb14283cfc81 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-fff6f95c-6c50-4564-8584-cb14283cfc81');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-cadf16bf-a000-4430-a2a8-c3ae7fe37a04\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cadf16bf-a000-4430-a2a8-c3ae7fe37a04')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-cadf16bf-a000-4430-a2a8-c3ae7fe37a04 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"labels_df","summary":"{\n  \"name\": \"labels_df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14433,\n        \"min\": 1,\n        \"max\": 50000,\n        \"num_unique_values\": 50000,\n        \"samples\": [\n          33554,\n          9428,\n          200\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"dog\",\n          \"truck\",\n          \"horse\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}],"source":["# 4. Análisis rápido de las etiquetas de trainLabels.csv\n","import pandas as pd\n","\n","# Cargar el CSV\n","labels_df = pd.read_csv('/content/cifar-10/trainLabels.csv')\n","\n","labels_df.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"a4-hz4KfSTfb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728416667578,"user_tz":360,"elapsed":11,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"5e8dbf42-7c84-43dc-deb5-6cab078e1c2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["100000\n"]}],"source":["print(labels_df.size)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pN3P1Uh3ifyy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728416674377,"user_tz":360,"elapsed":6806,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"cd4706af-7365-4b5b-b75b-8c1a25a5c187"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número de imágenes movidas a entrenamiento: 40000\n","Número de imágenes movidas a validación: 10000\n"]}],"source":["# 3. Crear listas de imágenes y etiquetas\n","from sklearn.model_selection import train_test_split\n","import shutil\n","import os\n","\n","# Asegurarse de que el ID esté correctamente formateado para coincidir con los nombres de archivo\n","\n","images = labels_df['id'].apply(lambda x: f'{x}.png').values\n","labels = labels_df['label'].values\n","\n","# Dividir en entrenamientos y validación\n","train_imgs, val_imgs, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n","\n","# Crear las carpetas de entrenamiento y validación\n","train_path = '/content/dataset/train'\n","val_path = '/content/dataset/val'\n","\n","# Crear caroetas por clase dentro de val y train\n","for label in labels_df[\"label\"].unique():\n","  os.makedirs(os.path.join(train_path, label), exist_ok=True)\n","  os.makedirs(os.path.join(val_path, label), exist_ok=True)\n","\n","train_count = 0\n","val_count = 0\n","\n","# Mover las imágenes de entrenamiento\n","for img, label in zip(train_imgs, train_labels):\n","    img_src = f'/content/cifar-10/train/train/{img}'\n","    img_dst = os.path.join(train_path, str(label), img)\n","    if os.path.exists(img_src):  # Verificar si la imagen existe\n","        shutil.move(img_src, img_dst)\n","        train_count += 1\n","\n","print(f'Número de imágenes movidas a entrenamiento: {train_count}')\n","\n","# Mover las imágenes de validación\n","for img, label in zip(val_imgs, val_labels):\n","    img_src = f'/content/cifar-10/train/train/{img}'\n","    img_dst = os.path.join(val_path, str(label), img)\n","    if os.path.exists(img_src):  # Verificar si la imagen existe\n","        shutil.move(img_src, img_dst)\n","        val_count += 1\n","\n","print(f'Número de imágenes movidas a validación: {val_count}')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"zaqjvfCHToSI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728416674378,"user_tz":360,"elapsed":20,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"22905a74-871f-4d91-eba2-d78900ca6352"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 4000\n"]}],"source":["# 3.1 Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/train/airplane'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"FJ1Bz5lYTuoY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728416674379,"user_tz":360,"elapsed":16,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"3bee475b-5f7d-4d0c-a001-3871e209559d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 4000\n"]}],"source":["# 3.2 Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/train/cat'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"HJDvAF8HTyt9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728416674379,"user_tz":360,"elapsed":12,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"dfd1befe-c8dd-43ce-f8ff-109f72601fb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 1000\n"]}],"source":["# 3.3 Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/val/deer'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"UVDUfDSaibek","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728416674601,"user_tz":360,"elapsed":231,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"ca63b1bf-e5e9-4822-d3dc-15a8a720e5fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["La carpeta no está vacía para train.\n","La carpeta no está vacía para val.\n"]}],"source":["# 6. Verificar que las imágenes se han movido de forma exitosa a las carpetas\n","import os\n","\n","def is_directory_empty(directory):\n","  return not os.listdir(directory)\n","\n","directory_train =  \"/content/dataset/train/airplane\"\n","\n","if is_directory_empty(directory_train):\n","  print(\"La carpeta está vacía para train.\")\n","else:\n","  print(\"La carpeta no está vacía para train.\")\n","\n","directory_val =  \"/content/dataset/val/airplane\"\n","\n","if is_directory_empty(directory_train):\n","  print(\"La carpeta está vacía para val.\")\n","else:\n","  print(\"La carpeta no está vacía para val.\")"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ClxtGVQKRQEt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728416674602,"user_tz":360,"elapsed":11,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"6812401c-2fc1-41ef-a896-e10bd2620a97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 4000\n"]}],"source":["# 3.1. Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/train/airplane'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"9GcZYwJ9Raff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728416674835,"user_tz":360,"elapsed":240,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"d9671ba4-a3fe-4c43-87f3-e3cddb4335d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 4000\n"]}],"source":["# 3.2 Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/train/automobile'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"GNdhtWNjRhdu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728416674835,"user_tz":360,"elapsed":15,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"ecb14795-2bfc-4bd5-d4a3-8abf6527eb80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 4000\n"]}],"source":["# 3.2 Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/train/bird'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"2VxfTVEeRtTl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728416674835,"user_tz":360,"elapsed":11,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"f7d8aa0b-b1ab-4464-a4d9-2fc99339befd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 1000\n"]}],"source":["# 3.1. Tamaño de elementos para carpeta VAL\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/val/airplane'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"b-IdX0v4lxjG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728419194187,"user_tz":360,"elapsed":2519358,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"84df38da-2d19-4606-f99c-aeeaffc13ada"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.8-py3-none-any.whl.metadata (34 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.9-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Downloading ultralytics-8.3.8-py3-none-any.whl (882 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m882.5/882.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.9-py3-none-any.whl (26 kB)\n","Installing collected packages: ultralytics-thop, ultralytics\n","Successfully installed ultralytics-8.3.8 ultralytics-thop-2.0.9\n","Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-cls.pt to 'yolov8n-cls.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5.31M/5.31M [00:00<00:00, 100MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.8 🚀 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/content/dataset, epochs=20, time=None, patience=100, batch=16, imgsz=32, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/dataset/train... found 40000 images in 10 classes ✅ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/dataset/val... found 10000 images in 10 classes ✅ \n","\u001b[34m\u001b[1mtest:\u001b[0m None...\n","Overriding model.yaml nc=1000 with nc=10\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    343050  ultralytics.nn.modules.head.Classify         [256, 10]                     \n","YOLOv8n-cls summary: 99 layers, 1,451,098 parameters, 1,451,098 gradients, 3.4 GFLOPs\n","Transferred 156/158 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLO11n...\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5.35M/5.35M [00:00<00:00, 98.6MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset/train... 40000 images, 0 corrupt: 100%|██████████| 40000/40000 [00:07<00:00, 5127.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/dataset/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/val... 10000 images, 0 corrupt: 100%|██████████| 10000/10000 [00:01<00:00, 5045.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/dataset/val.cache\n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n","Image sizes 32 train, 32 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/classify/train\u001b[0m\n","Starting training for 20 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       1/20     0.132G      2.563         16         32:   0%|          | 9/2500 [00:01<04:07, 10.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"output_type":"stream","name":"stderr","text":["       1/20     0.132G      2.606         16         32:   1%|          | 21/2500 [00:02<03:09, 13.10it/s]\n","100%|██████████| 755k/755k [00:00<00:00, 19.8MB/s]\n","       1/20     0.134G      1.959         16         32: 100%|██████████| 2500/2500 [02:15<00:00, 18.41it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:08<00:00, 35.27it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.508      0.927\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       2/20     0.109G      1.831         16         32: 100%|██████████| 2500/2500 [01:55<00:00, 21.72it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:08<00:00, 37.00it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.522      0.936\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       3/20     0.109G      1.778         16         32: 100%|██████████| 2500/2500 [01:53<00:00, 21.93it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:08<00:00, 35.54it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.586      0.945\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       4/20     0.109G       1.56         16         32: 100%|██████████| 2500/2500 [01:48<00:00, 22.97it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:06<00:00, 45.36it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.653       0.97\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       5/20     0.109G      1.395         16         32: 100%|██████████| 2500/2500 [01:53<00:00, 21.95it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:05<00:00, 58.38it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.671       0.97\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       6/20     0.109G      1.328         16         32: 100%|██████████| 2500/2500 [01:52<00:00, 22.21it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:06<00:00, 48.56it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.689      0.971\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       7/20     0.109G      1.284         16         32: 100%|██████████| 2500/2500 [01:49<00:00, 22.75it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:07<00:00, 42.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all       0.69      0.975\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       8/20     0.109G      1.243         16         32: 100%|██████████| 2500/2500 [01:52<00:00, 22.24it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:05<00:00, 52.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.696       0.98\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       9/20     0.109G        1.2         16         32: 100%|██████████| 2500/2500 [01:53<00:00, 22.05it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:05<00:00, 58.40it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.711      0.979\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      10/20     0.109G      1.162         16         32: 100%|██████████| 2500/2500 [01:54<00:00, 21.79it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:05<00:00, 53.81it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.734      0.982\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      11/20     0.109G      1.133         16         32: 100%|██████████| 2500/2500 [01:53<00:00, 22.07it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:06<00:00, 45.14it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.727      0.981\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      12/20     0.109G      1.115         16         32: 100%|██████████| 2500/2500 [01:49<00:00, 22.84it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:08<00:00, 37.95it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.731      0.979\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      13/20     0.109G      1.111         16         32: 100%|██████████| 2500/2500 [01:48<00:00, 23.00it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:06<00:00, 46.99it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.728      0.981\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      14/20     0.109G      1.072         16         32: 100%|██████████| 2500/2500 [01:52<00:00, 22.14it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:05<00:00, 56.50it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.738      0.983\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      15/20     0.109G      1.047         16         32: 100%|██████████| 2500/2500 [01:59<00:00, 20.85it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:07<00:00, 42.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.754      0.984\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      16/20     0.109G      1.036         16         32: 100%|██████████| 2500/2500 [01:59<00:00, 20.90it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:07<00:00, 40.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.759      0.984\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      17/20     0.109G       1.01         16         32: 100%|██████████| 2500/2500 [01:58<00:00, 21.15it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:08<00:00, 38.78it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.757      0.985\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      18/20     0.109G       0.98         16         32: 100%|██████████| 2500/2500 [02:01<00:00, 20.61it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:06<00:00, 50.04it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.751      0.986\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      19/20     0.109G     0.9646         16         32: 100%|██████████| 2500/2500 [02:01<00:00, 20.58it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:05<00:00, 57.73it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.743      0.985\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      20/20     0.109G     0.9379         16         32: 100%|██████████| 2500/2500 [02:04<00:00, 20.11it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:06<00:00, 47.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.736      0.985\n","\n","20 epochs completed in 0.687 hours.\n","Optimizer stripped from runs/classify/train/weights/last.pt, 3.0MB\n","Optimizer stripped from runs/classify/train/weights/best.pt, 3.0MB\n","\n","Validating runs/classify/train/weights/best.pt...\n","Ultralytics 8.3.8 🚀 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv8n-cls summary (fused): 73 layers, 1,447,690 parameters, 0 gradients, 3.3 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/dataset/train... found 40000 images in 10 classes ✅ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/dataset/val... found 10000 images in 10 classes ✅ \n","\u001b[34m\u001b[1mtest:\u001b[0m None...\n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:05<00:00, 53.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.759      0.984\n","Speed: 0.0ms preprocess, 0.3ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/train\u001b[0m\n","Results saved to \u001b[1mruns/classify/train\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n","\n","confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7b0a4ae43220>\n","curves: []\n","curves_results: []\n","fitness: 0.8716000020503998\n","keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n","results_dict: {'metrics/accuracy_top1': 0.7590000033378601, 'metrics/accuracy_top5': 0.9842000007629395, 'fitness': 0.8716000020503998}\n","save_dir: PosixPath('runs/classify/train')\n","speed: {'preprocess': 0.011421680450439453, 'inference': 0.30826783180236816, 'loss': 0.0004799842834472656, 'postprocess': 0.0005187034606933595}\n","task: 'classify'\n","top1: 0.7590000033378601\n","top5: 0.9842000007629395\n","training: {'epochs': 20, 'seconds': 2472.8527204990387}"]},"metadata":{},"execution_count":15}],"source":["### ENTRENAMIENTO DEL MODELO ####\n","# 7. Entrenar el modelo\n","!pip install ultralytics\n","from ultralytics import YOLO\n","\n","# Cargar el modelo yolov8n-cls.pt para clasificación\n","model = YOLO('yolov8n-cls.pt')\n","# Entrenar el modelo con CIFAR-10\n","model.train(data='/content/dataset', epochs=20, imgsz=32)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"qGusSGqxnA-w","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1cuoe8PhAOr80ML5l4dWNCRuKD6VYTpVz"},"executionInfo":{"status":"ok","timestamp":1728419207888,"user_tz":360,"elapsed":13724,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"10061542-9074-4d79-e5c6-a66a357f761d"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# 8. Visualizar los resultados de las gráficas YOLO\n","import os\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# Ruta del directorio donde se encuentran las imágenes\n","directory = \"/content/runs/classify/train\"\n","\n","# Listar todos los archivos en el directorio\n","files = os.listdir(directory)\n","print(\"Estos son los archivos de resultado de entrenamiento\", files)\n","\n","# Filtrar solo los archivos de imagen (extensiones comunes)\n","image_files = [file for file in files if file.endswith(('.png', '.jpg', '.jpeg'))]\n","\n","# Mostrar todas las imágenes\n","# Mostrar todas las imágenes\n","if image_files:\n","    for image_file in image_files:\n","        image_path = os.path.join(directory, image_file)\n","\n","        # Cargar la imagen\n","        image = Image.open(image_path)\n","\n","        # Mostrar la imagen\n","        plt.figure()\n","        plt.imshow(image)\n","        plt.axis('off')  # Ocultar los ejes\n","        plt.title(image_file)  # Título con el nombre del archivo\n","    plt.show()\n","else:\n","    print(\"No se encontraron imágenes en la carpeta.\")"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"kZYgY-H7pMjn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728419207889,"user_tz":360,"elapsed":27,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"786ccd9c-5779-4c95-e4b5-b9cedec43a2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 airplane 0.97, bird 0.01, automobile 0.01, ship 0.01, cat 0.00, 7.6ms\n","Speed: 12.0ms preprocess, 7.6ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train2\u001b[0m\n"]}],"source":["# 9. Predicción con imagen del dataset\n","import os\n","from PIL import Image\n","\n","directory = \"/content/dataset/train/airplane\"\n","\n","# Si la carpeta tiene imágenes, carga la primera imagen\n","image_path = os.path.join(directory, os.listdir(directory)[0])\n","image = Image.open(image_path)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"Dl4Zc03253sg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728419209452,"user_tz":360,"elapsed":1582,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"4cdc3f9f-3f4c-45af-ece3-a15a5d073a84"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-10-08 20:26:47--  https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/rana.jpg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 107572 (105K) [image/jpeg]\n","Saving to: ‘rana.jpg’\n","\n","rana.jpg            100%[===================>] 105.05K  --.-KB/s    in 0.02s   \n","\n","2024-10-08 20:26:48 (6.22 MB/s) - ‘rana.jpg’ saved [107572/107572]\n","\n","\n","0: 32x32 frog 0.66, bird 0.24, airplane 0.03, deer 0.02, horse 0.01, 5.9ms\n","Speed: 14.4ms preprocess, 5.9ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train3\u001b[0m\n"]}],"source":["!wget https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/rana.jpg\n","\n","# 10.1  Predicción con imagen de internet RANA\n","import os\n","from PIL import Image\n","\n","directory = \"/content/rana.jpg\"\n","\n","image = Image.open(directory)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/cauto_test.jpg\n","\n","# 10.1  Predicción con imagen de internet RANA\n","import os\n","from PIL import Image\n","\n","directory = \"/content/cauto_test.jpg\"\n","\n","image = Image.open(directory)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FCEbWO_mt05p","executionInfo":{"status":"ok","timestamp":1728419209453,"user_tz":360,"elapsed":25,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"7fc088b9-9986-4736-a429-3203e284a94c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-10-08 20:26:48--  https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/cauto_test.jpg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 350294 (342K) [image/jpeg]\n","Saving to: ‘cauto_test.jpg’\n","\n","\rcauto_test.jpg        0%[                    ]       0  --.-KB/s               \rcauto_test.jpg      100%[===================>] 342.08K  --.-KB/s    in 0.03s   \n","\n","2024-10-08 20:26:48 (12.6 MB/s) - ‘cauto_test.jpg’ saved [350294/350294]\n","\n","\n","0: 32x32 automobile 0.96, truck 0.02, airplane 0.01, ship 0.01, deer 0.00, 3.0ms\n","Speed: 4.3ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train4\u001b[0m\n"]}]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/Pelusa.jpeg\n","\n","# 10.1  Predicción con imagen de internet RANA\n","import os\n","from PIL import Image\n","\n","directory = \"/content/Pelusa.jpeg\"\n","\n","image = Image.open(directory)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTrPFWrgt1EX","executionInfo":{"status":"ok","timestamp":1728419210905,"user_tz":360,"elapsed":1463,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"86879060-324b-4352-b562-0504bbe4e849"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-10-08 20:26:49--  https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/Pelusa.jpeg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 239195 (234K) [image/jpeg]\n","Saving to: ‘Pelusa.jpeg’\n","\n","\rPelusa.jpeg           0%[                    ]       0  --.-KB/s               \rPelusa.jpeg         100%[===================>] 233.59K  --.-KB/s    in 0.02s   \n","\n","2024-10-08 20:26:49 (10.2 MB/s) - ‘Pelusa.jpeg’ saved [239195/239195]\n","\n","\n","0: 32x32 horse 0.52, cat 0.16, deer 0.12, truck 0.07, airplane 0.05, 5.8ms\n","Speed: 22.9ms preprocess, 5.8ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train5\u001b[0m\n"]}]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/gato3.JPG\n","\n","# 10.1  Predicción con imagen de internet RANA\n","import os\n","from PIL import Image\n","\n","directory = \"/content/gato3.JPG\"\n","\n","image = Image.open(directory)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPvNEfHat1JF","executionInfo":{"status":"ok","timestamp":1728419211366,"user_tz":360,"elapsed":472,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"b721ae31-3563-4058-d8ea-50ba695fda64"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-10-08 20:26:50--  https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/gato3.JPG\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 88203 (86K) [image/jpeg]\n","Saving to: ‘gato3.JPG’\n","\n","gato3.JPG           100%[===================>]  86.14K  --.-KB/s    in 0.01s   \n","\n","2024-10-08 20:26:50 (6.05 MB/s) - ‘gato3.JPG’ saved [88203/88203]\n","\n","\n","0: 32x32 cat 0.50, dog 0.40, horse 0.03, bird 0.03, deer 0.02, 5.3ms\n","Speed: 9.2ms preprocess, 5.3ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train6\u001b[0m\n"]}]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/Minicky.jpeg\n","\n","# 10.1  Predicción con imagen de internet RANA\n","import os\n","from PIL import Image\n","\n","directory = \"/content/Minicky.jpeg\"\n","\n","image = Image.open(directory)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZ4Df2qht1Mb","executionInfo":{"status":"ok","timestamp":1728419213033,"user_tz":360,"elapsed":1676,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"d2a7b2f0-9641-4891-acf5-fa6efb2452e4"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-10-08 20:26:51--  https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/Minicky.jpeg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 213012 (208K) [image/jpeg]\n","Saving to: ‘Minicky.jpeg’\n","\n","Minicky.jpeg        100%[===================>] 208.02K  --.-KB/s    in 0.02s   \n","\n","2024-10-08 20:26:51 (8.21 MB/s) - ‘Minicky.jpeg’ saved [213012/213012]\n","\n","\n","0: 32x32 cat 0.33, ship 0.20, automobile 0.13, frog 0.12, airplane 0.09, 4.8ms\n","Speed: 34.6ms preprocess, 4.8ms inference, 3.2ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train7\u001b[0m\n"]}]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/Pelusa_zoom.png\n","\n","# 10.1  Predicción con imagen de internet RANA\n","import os\n","from PIL import Image\n","\n","directory = \"/content/Pelusa_zoom.png\"\n","\n","image = Image.open(directory)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYkCudbit1Po","executionInfo":{"status":"ok","timestamp":1728419213901,"user_tz":360,"elapsed":876,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"f46a5ec7-f3fd-41df-de7e-61b952a6f41e"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-10-08 20:26:52--  https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/Pelusa_zoom.png\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 443434 (433K) [image/png]\n","Saving to: ‘Pelusa_zoom.png’\n","\n","Pelusa_zoom.png     100%[===================>] 433.04K  --.-KB/s    in 0.04s   \n","\n","2024-10-08 20:26:53 (10.3 MB/s) - ‘Pelusa_zoom.png’ saved [443434/443434]\n","\n","\n","0: 32x32 cat 0.39, frog 0.33, ship 0.14, truck 0.03, dog 0.03, 3.6ms\n","Speed: 4.3ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train8\u001b[0m\n"]}]},{"cell_type":"code","execution_count":24,"metadata":{"id":"sIliKVVusyBW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728419236260,"user_tz":360,"elapsed":22367,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"461b2d7e-a0fe-4518-81a7-bb9297c0905f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.8 🚀 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv8n-cls summary (fused): 73 layers, 1,447,690 parameters, 0 gradients, 3.3 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/dataset/train... found 40000 images in 10 classes ✅ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/dataset/val... found 10000 images in 10 classes ✅ \n","\u001b[34m\u001b[1mtest:\u001b[0m None...\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/val... 10000 images, 0 corrupt: 100%|██████████| 10000/10000 [00:00<?, ?it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 625/625 [00:16<00:00, 38.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.759      0.984\n","Speed: 0.0ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/val\u001b[0m\n","Top-1 Accuracy: 0.7590000033378601\n","Top-5 Accuracy: 0.9842000007629395\n","Fitness: 0.8716000020503998\n","\n","Todas las métricas:\n","metrics/accuracy_top1: 0.7590000033378601\n","metrics/accuracy_top5: 0.9842000007629395\n","fitness: 0.8716000020503998\n"]}],"source":["# 11. Validar el modelo\n","from ultralytics import YOLO\n","\n","# Cargar el modelo\n","model = YOLO(\"/content/runs/classify/train/weights/best.pt\")\n","\n","# Validar el modelo\n","metrics = model.val()\n","\n","# Acceder a las métricas de clasificación\n","print(f\"Top-1 Accuracy: {metrics.top1}\")\n","print(f\"Top-5 Accuracy: {metrics.top5}\")\n","print(f\"Fitness: {metrics.fitness}\")\n","\n","# Si quieres ver todas las métricas disponibles\n","print(\"\\nTodas las métricas:\")\n","for key, value in metrics.results_dict.items():\n","    print(f\"{key}: {value}\")"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"fiV6d370yipm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728419237518,"user_tz":360,"elapsed":1273,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"50e40d6a-cf5e-4ab7-cf9f-5329d3e1cb92"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 horse 0.52, cat 0.16, deer 0.12, truck 0.07, airplane 0.05, 3.7ms\n","Speed: 23.6ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/predict\u001b[0m\n","Clase: horse, Confianza: 0.52\n","airplane: 0.0524\n","automobile: 0.0169\n","bird: 0.0046\n","cat: 0.1600\n","deer: 0.1170\n","dog: 0.0491\n","frog: 0.0020\n","horse: 0.5229\n","ship: 0.0046\n","truck: 0.0705\n"]}],"source":["import os\n","from PIL import Image\n","from ultralytics import YOLO  # Asegúrate de importar YOLO si no lo has hecho ya\n","\n","# Ruta del modelo YOLO\n","model_path = r'/content/runs/classify/train/weights/best.pt'\n","\n","# Cargar el modelo YOLO\n","model = YOLO(model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Pelusa.jpeg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Realizar la predicción con el modelo YOLO usando predict() y un umbral de confianza de 0.25\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones (si se ha guardado o procesado)\n","image.show()\n","\n","# Procesar y mostrar los resultados de la predicción\n","for result in results:\n","    names = result.names  # Nombres de las clases\n","    top_class = result.probs.top1  # Índice de la clase con mayor probabilidad\n","    top_confidence = result.probs.top1conf.item()  # Probabilidad de la clase con mayor probabilidad\n","\n","    print(f\"Clase: {names[top_class]}, Confianza: {top_confidence:.2f}\")\n","\n","    # Si quieres mostrar todas las probabilidades:\n","    for i, prob in enumerate(result.probs.data):\n","        print(f\"{names[i]}: {prob:.4f}\")"]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from ultralytics import YOLO  # Asegúrate de importar YOLO si no lo has hecho ya\n","\n","# Ruta del modelo YOLO\n","model_path = r'/content/runs/classify/train/weights/best.pt'\n","\n","# Cargar el modelo YOLO\n","model = YOLO(model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Pelusa_zoom.png'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Realizar la predicción con el modelo YOLO usando predict() y un umbral de confianza de 0.25\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones (si se ha guardado o procesado)\n","image.show()\n","\n","# Procesar y mostrar los resultados de la predicción\n","for result in results:\n","    names = result.names  # Nombres de las clases\n","    top_class = result.probs.top1  # Índice de la clase con mayor probabilidad\n","    top_confidence = result.probs.top1conf.item()  # Probabilidad de la clase con mayor probabilidad\n","\n","    print(f\"Clase: {names[top_class]}, Confianza: {top_confidence:.2f}\")\n","\n","    # Si quieres mostrar todas las probabilidades:\n","    for i, prob in enumerate(result.probs.data):\n","        print(f\"{names[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwag8JyLv6Ni","executionInfo":{"status":"ok","timestamp":1728419237859,"user_tz":360,"elapsed":351,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"9a1308e9-c8b1-4394-c8f6-d92b0c6c5cfc"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 cat 0.39, frog 0.33, ship 0.14, truck 0.03, dog 0.03, 4.8ms\n","Speed: 4.5ms preprocess, 4.8ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/predict2\u001b[0m\n","Clase: cat, Confianza: 0.39\n","airplane: 0.0064\n","automobile: 0.0232\n","bird: 0.0276\n","cat: 0.3906\n","deer: 0.0120\n","dog: 0.0277\n","frog: 0.3279\n","horse: 0.0151\n","ship: 0.1388\n","truck: 0.0307\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from ultralytics import YOLO  # Asegúrate de importar YOLO si no lo has hecho ya\n","\n","# Ruta del modelo YOLO\n","model_path = r'/content/runs/classify/train/weights/best.pt'\n","\n","# Cargar el modelo YOLO\n","model = YOLO(model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/cauto_test.jpg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Realizar la predicción con el modelo YOLO usando predict() y un umbral de confianza de 0.25\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones (si se ha guardado o procesado)\n","image.show()\n","\n","# Procesar y mostrar los resultados de la predicción\n","for result in results:\n","    names = result.names  # Nombres de las clases\n","    top_class = result.probs.top1  # Índice de la clase con mayor probabilidad\n","    top_confidence = result.probs.top1conf.item()  # Probabilidad de la clase con mayor probabilidad\n","\n","    print(f\"Clase: {names[top_class]}, Confianza: {top_confidence:.2f}\")\n","\n","    # Si quieres mostrar todas las probabilidades:\n","    for i, prob in enumerate(result.probs.data):\n","        print(f\"{names[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oepeuuHv84B","executionInfo":{"status":"ok","timestamp":1728419238933,"user_tz":360,"elapsed":1086,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"bb150189-934b-4726-b3a7-e6415a961fcb"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 automobile 0.96, truck 0.02, airplane 0.01, ship 0.01, deer 0.00, 5.2ms\n","Speed: 14.3ms preprocess, 5.2ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/predict3\u001b[0m\n","Clase: automobile, Confianza: 0.96\n","airplane: 0.0073\n","automobile: 0.9616\n","bird: 0.0008\n","cat: 0.0006\n","deer: 0.0016\n","dog: 0.0003\n","frog: 0.0013\n","horse: 0.0003\n","ship: 0.0051\n","truck: 0.0210\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from ultralytics import YOLO  # Asegúrate de importar YOLO si no lo has hecho ya\n","\n","# Ruta del modelo YOLO\n","model_path = r'/content/runs/classify/train/weights/best.pt'\n","\n","# Cargar el modelo YOLO\n","model = YOLO(model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/rana.jpg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Realizar la predicción con el modelo YOLO usando predict() y un umbral de confianza de 0.25\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones (si se ha guardado o procesado)\n","image.show()\n","\n","# Procesar y mostrar los resultados de la predicción\n","for result in results:\n","    names = result.names  # Nombres de las clases\n","    top_class = result.probs.top1  # Índice de la clase con mayor probabilidad\n","    top_confidence = result.probs.top1conf.item()  # Probabilidad de la clase con mayor probabilidad\n","\n","    print(f\"Clase: {names[top_class]}, Confianza: {top_confidence:.2f}\")\n","\n","    # Si quieres mostrar todas las probabilidades:\n","    for i, prob in enumerate(result.probs.data):\n","        print(f\"{names[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ase8qq8tv_-S","executionInfo":{"status":"ok","timestamp":1728419239660,"user_tz":360,"elapsed":776,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"31e0a6bd-096a-49e0-f48f-7f99c98d4860"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 frog 0.66, bird 0.24, airplane 0.03, deer 0.02, horse 0.01, 4.3ms\n","Speed: 15.1ms preprocess, 4.3ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/predict4\u001b[0m\n","Clase: frog, Confianza: 0.66\n","airplane: 0.0325\n","automobile: 0.0025\n","bird: 0.2403\n","cat: 0.0106\n","deer: 0.0230\n","dog: 0.0067\n","frog: 0.6647\n","horse: 0.0146\n","ship: 0.0018\n","truck: 0.0033\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from ultralytics import YOLO  # Asegúrate de importar YOLO si no lo has hecho ya\n","\n","# Ruta del modelo YOLO\n","model_path = r'/content/runs/classify/train/weights/best.pt'\n","\n","# Cargar el modelo YOLO\n","model = YOLO(model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/gato3.JPG'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Realizar la predicción con el modelo YOLO usando predict() y un umbral de confianza de 0.25\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones (si se ha guardado o procesado)\n","image.show()\n","\n","# Procesar y mostrar los resultados de la predicción\n","for result in results:\n","    names = result.names  # Nombres de las clases\n","    top_class = result.probs.top1  # Índice de la clase con mayor probabilidad\n","    top_confidence = result.probs.top1conf.item()  # Probabilidad de la clase con mayor probabilidad\n","\n","    print(f\"Clase: {names[top_class]}, Confianza: {top_confidence:.2f}\")\n","\n","    # Si quieres mostrar todas las probabilidades:\n","    for i, prob in enumerate(result.probs.data):\n","        print(f\"{names[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FMAxdNgRwDZD","executionInfo":{"status":"ok","timestamp":1728419240548,"user_tz":360,"elapsed":893,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"1a6f8827-709e-412c-e096-3aee3470b24b"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 cat 0.50, dog 0.40, horse 0.03, bird 0.03, deer 0.02, 16.6ms\n","Speed: 22.0ms preprocess, 16.6ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/predict5\u001b[0m\n","Clase: cat, Confianza: 0.50\n","airplane: 0.0018\n","automobile: 0.0014\n","bird: 0.0293\n","cat: 0.4951\n","deer: 0.0225\n","dog: 0.3952\n","frog: 0.0163\n","horse: 0.0332\n","ship: 0.0030\n","truck: 0.0023\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from ultralytics import YOLO  # Asegúrate de importar YOLO si no lo has hecho ya\n","\n","# Ruta del modelo YOLO\n","model_path = r'/content/runs/classify/train/weights/best.pt'\n","\n","# Cargar el modelo YOLO\n","model = YOLO(model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Minicky.jpeg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Realizar la predicción con el modelo YOLO usando predict() y un umbral de confianza de 0.25\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones (si se ha guardado o procesado)\n","image.show()\n","\n","# Procesar y mostrar los resultados de la predicción\n","for result in results:\n","    names = result.names  # Nombres de las clases\n","    top_class = result.probs.top1  # Índice de la clase con mayor probabilidad\n","    top_confidence = result.probs.top1conf.item()  # Probabilidad de la clase con mayor probabilidad\n","\n","    print(f\"Clase: {names[top_class]}, Confianza: {top_confidence:.2f}\")\n","\n","    # Si quieres mostrar todas las probabilidades:\n","    for i, prob in enumerate(result.probs.data):\n","        print(f\"{names[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HFPZjEg0wELu","executionInfo":{"status":"ok","timestamp":1728419242794,"user_tz":360,"elapsed":2253,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"4bcc4dc7-7dbf-4717-ba30-b391d8dac2bb"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 cat 0.33, ship 0.20, automobile 0.13, frog 0.12, airplane 0.09, 8.9ms\n","Speed: 37.2ms preprocess, 8.9ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/predict6\u001b[0m\n","Clase: cat, Confianza: 0.33\n","airplane: 0.0928\n","automobile: 0.1290\n","bird: 0.0859\n","cat: 0.3290\n","deer: 0.0059\n","dog: 0.0166\n","frog: 0.1152\n","horse: 0.0071\n","ship: 0.2011\n","truck: 0.0173\n"]}]},{"cell_type":"code","source":["# 14. Exportar modelo a onnx\n","from ultralytics import YOLO\n","\n","model = YOLO(r'/content/runs/classify/train/weights/best.pt')  # load a custom trained model\n","\n","# Export the model\n","model.export(format='onnx', opset=12, simplify=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":852},"id":"y8A_mzcAwGmU","executionInfo":{"status":"ok","timestamp":1728419262151,"user_tz":360,"elapsed":19370,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"86504c33-b47d-40f5-f235-1fa50a42f0dd"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.8 🚀 Python-3.10.12 torch-2.4.1+cu121 CPU (Intel Xeon 2.00GHz)\n","YOLOv8n-cls summary (fused): 73 layers, 1,447,690 parameters, 0 gradients, 3.3 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/runs/classify/train/weights/best.pt' with input shape (1, 3, 32, 32) BCHW and output shape(s) (1, 10) (2.8 MB)\n","\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0', 'onnxslim==0.1.34', 'onnxruntime-gpu'] not found, attempting AutoUpdate...\n","Collecting onnx>=1.12.0\n","  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Collecting onnxslim==0.1.34\n","  Downloading onnxslim-0.1.34-py3-none-any.whl.metadata (2.7 kB)\n","Collecting onnxruntime-gpu\n","  Downloading onnxruntime_gpu-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxslim==0.1.34) (1.13.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxslim==0.1.34) (24.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (1.26.4)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (3.20.3)\n","Collecting coloredlogs (from onnxruntime-gpu)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.3.25)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxslim==0.1.34) (1.3.0)\n","Downloading onnxslim-0.1.34-py3-none-any.whl (140 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.3/140.3 kB 10.2 MB/s eta 0:00:00\n","Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.0/16.0 MB 263.9 MB/s eta 0:00:00\n","Downloading onnxruntime_gpu-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (226.2 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.2/226.2 MB 232.2 MB/s eta 0:00:00\n","Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 215.5 MB/s eta 0:00:00\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 180.3 MB/s eta 0:00:00\n","Installing collected packages: onnx, humanfriendly, onnxslim, coloredlogs, onnxruntime-gpu\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-gpu-1.19.2 onnxslim-0.1.34\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 16.9s, installed 3 packages: ['onnx>=1.12.0', 'onnxslim==0.1.34', 'onnxruntime-gpu']\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n","\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.34...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 17.4s, saved as '/content/runs/classify/train/weights/best.onnx' (5.5 MB)\n","\n","Export complete (19.3s)\n","Results saved to \u001b[1m/content/runs/classify/train/weights\u001b[0m\n","Predict:         yolo predict task=classify model=/content/runs/classify/train/weights/best.onnx imgsz=32  \n","Validate:        yolo val task=classify model=/content/runs/classify/train/weights/best.onnx imgsz=32 data=/content/dataset  \n","Visualize:       https://netron.app\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/runs/classify/train/weights/best.onnx'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["import onnxruntime as ort\n","from PIL import Image\n","import numpy as np\n","import os\n","\n","# Ruta del modelo ONNX exportado\n","onnx_model_path = r'/content/runs/classify/train/weights/best.onnx'\n","\n","# Cargar el modelo ONNX usando onnxruntime\n","session = ort.InferenceSession(onnx_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/cauto_test.jpg'  # Cambia esto a la ruta real de tu imagen\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Redimensionar la imagen al tamaño adecuado para CIFAR-10 (32x32)\n","image = image.resize((32, 32))\n","\n","# Convertir la imagen a un array numpy y escalar los valores de píxeles\n","image_np = np.array(image).astype(np.float32) / 255.0\n","\n","# Cambiar el formato de la imagen a (1, 3, 32, 32) -> (batch_size, canales, alto, ancho)\n","image_np = np.transpose(image_np, (2, 0, 1))  # Cambiar los ejes para que esté en formato canales primero\n","image_np = np.expand_dims(image_np, axis=0)   # Añadir la dimensión de batch_size\n","\n","# Realizar la predicción con el modelo ONNX\n","input_name = session.get_inputs()[0].name  # Obtener el nombre del primer input del modelo\n","output_name = session.get_outputs()[0].name  # Obtener el nombre del primer output del modelo\n","results = session.run([output_name], {input_name: image_np})\n","\n","# Procesar los resultados de la predicción\n","predicciones = results[0][0]  # Acceder a las predicciones de la primera imagen\n","clase_detectada = np.argmax(predicciones)  # Obtener la clase con mayor probabilidad\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOkueZsfwL9l","executionInfo":{"status":"ok","timestamp":1728419262151,"user_tz":360,"elapsed":23,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"d0975a49-6536-4f38-9551-95bd403392fa"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Primera opción detectada: truck con probabilidad 0.47\n","airplane: 0.1169\n","automobile: 0.3040\n","bird: 0.0044\n","cat: 0.0068\n","deer: 0.0151\n","dog: 0.0020\n","frog: 0.0105\n","horse: 0.0067\n","ship: 0.0651\n","truck: 0.4684\n"]}]},{"cell_type":"code","source":["import onnxruntime as ort\n","from PIL import Image\n","import numpy as np\n","import os\n","\n","# Ruta del modelo ONNX exportado\n","onnx_model_path = r'/content/runs/classify/train/weights/best.onnx'\n","\n","# Cargar el modelo ONNX usando onnxruntime\n","session = ort.InferenceSession(onnx_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/rana.jpg'  # Cambia esto a la ruta real de tu imagen\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Redimensionar la imagen al tamaño adecuado para CIFAR-10 (32x32)\n","image = image.resize((32, 32))\n","\n","# Convertir la imagen a un array numpy y escalar los valores de píxeles\n","image_np = np.array(image).astype(np.float32) / 255.0\n","\n","# Cambiar el formato de la imagen a (1, 3, 32, 32) -> (batch_size, canales, alto, ancho)\n","image_np = np.transpose(image_np, (2, 0, 1))  # Cambiar los ejes para que esté en formato canales primero\n","image_np = np.expand_dims(image_np, axis=0)   # Añadir la dimensión de batch_size\n","\n","# Realizar la predicción con el modelo ONNX\n","input_name = session.get_inputs()[0].name  # Obtener el nombre del primer input del modelo\n","output_name = session.get_outputs()[0].name  # Obtener el nombre del primer output del modelo\n","results = session.run([output_name], {input_name: image_np})\n","\n","# Procesar los resultados de la predicción\n","predicciones = results[0][0]  # Acceder a las predicciones de la primera imagen\n","clase_detectada = np.argmax(predicciones)  # Obtener la clase con mayor probabilidad\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sPfyBSkKwhUe","executionInfo":{"status":"ok","timestamp":1728419262473,"user_tz":360,"elapsed":338,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"88d9950e-aef5-434a-8938-80f7207c5c7f"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Primera opción detectada: frog con probabilidad 0.71\n","airplane: 0.0054\n","automobile: 0.0010\n","bird: 0.2343\n","cat: 0.0122\n","deer: 0.0198\n","dog: 0.0079\n","frog: 0.7063\n","horse: 0.0108\n","ship: 0.0005\n","truck: 0.0018\n"]}]},{"cell_type":"code","source":["import onnxruntime as ort\n","from PIL import Image\n","import numpy as np\n","import os\n","\n","# Ruta del modelo ONNX exportado\n","onnx_model_path = r'/content/runs/classify/train/weights/best.onnx'\n","\n","# Cargar el modelo ONNX usando onnxruntime\n","session = ort.InferenceSession(onnx_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Pelusa.jpeg'  # Cambia esto a la ruta real de tu imagen\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Redimensionar la imagen al tamaño adecuado para CIFAR-10 (32x32)\n","image = image.resize((32, 32))\n","\n","# Convertir la imagen a un array numpy y escalar los valores de píxeles\n","image_np = np.array(image).astype(np.float32) / 255.0\n","\n","# Cambiar el formato de la imagen a (1, 3, 32, 32) -> (batch_size, canales, alto, ancho)\n","image_np = np.transpose(image_np, (2, 0, 1))  # Cambiar los ejes para que esté en formato canales primero\n","image_np = np.expand_dims(image_np, axis=0)   # Añadir la dimensión de batch_size\n","\n","# Realizar la predicción con el modelo ONNX\n","input_name = session.get_inputs()[0].name  # Obtener el nombre del primer input del modelo\n","output_name = session.get_outputs()[0].name  # Obtener el nombre del primer output del modelo\n","results = session.run([output_name], {input_name: image_np})\n","\n","# Procesar los resultados de la predicción\n","predicciones = results[0][0]  # Acceder a las predicciones de la primera imagen\n","clase_detectada = np.argmax(predicciones)  # Obtener la clase con mayor probabilidad\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WacwJLW1wjwp","executionInfo":{"status":"ok","timestamp":1728419262473,"user_tz":360,"elapsed":17,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"3b959d9e-86b3-4fef-d226-b2d12126fdd8"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Primera opción detectada: horse con probabilidad 0.36\n","airplane: 0.1804\n","automobile: 0.0694\n","bird: 0.0062\n","cat: 0.0539\n","deer: 0.0220\n","dog: 0.0241\n","frog: 0.0016\n","horse: 0.3556\n","ship: 0.0348\n","truck: 0.2521\n"]}]},{"cell_type":"code","source":["import onnxruntime as ort\n","from PIL import Image\n","import numpy as np\n","import os\n","\n","# Ruta del modelo ONNX exportado\n","onnx_model_path = r'/content/runs/classify/train/weights/best.onnx'\n","\n","# Cargar el modelo ONNX usando onnxruntime\n","session = ort.InferenceSession(onnx_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Pelusa_zoom.png'  # Cambia esto a la ruta real de tu imagen\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Redimensionar la imagen al tamaño adecuado para CIFAR-10 (32x32)\n","image = image.resize((32, 32))\n","\n","# Convertir la imagen a un array numpy y escalar los valores de píxeles\n","image_np = np.array(image).astype(np.float32) / 255.0\n","\n","# Cambiar el formato de la imagen a (1, 3, 32, 32) -> (batch_size, canales, alto, ancho)\n","image_np = np.transpose(image_np, (2, 0, 1))  # Cambiar los ejes para que esté en formato canales primero\n","image_np = np.expand_dims(image_np, axis=0)   # Añadir la dimensión de batch_size\n","\n","# Realizar la predicción con el modelo ONNX\n","input_name = session.get_inputs()[0].name  # Obtener el nombre del primer input del modelo\n","output_name = session.get_outputs()[0].name  # Obtener el nombre del primer output del modelo\n","results = session.run([output_name], {input_name: image_np})\n","\n","# Procesar los resultados de la predicción\n","predicciones = results[0][0]  # Acceder a las predicciones de la primera imagen\n","clase_detectada = np.argmax(predicciones)  # Obtener la clase con mayor probabilidad\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JEEUOv-zwnA0","executionInfo":{"status":"ok","timestamp":1728419262474,"user_tz":360,"elapsed":12,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"149cad44-fffe-4102-bf54-8b8ef216c7f6"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Primera opción detectada: cat con probabilidad 0.42\n","airplane: 0.0053\n","automobile: 0.0146\n","bird: 0.0265\n","cat: 0.4190\n","deer: 0.0107\n","dog: 0.0365\n","frog: 0.3549\n","horse: 0.0225\n","ship: 0.0891\n","truck: 0.0210\n"]}]},{"cell_type":"code","source":["import onnxruntime as ort\n","from PIL import Image\n","import numpy as np\n","import os\n","\n","# Ruta del modelo ONNX exportado\n","onnx_model_path = r'/content/runs/classify/train/weights/best.onnx'\n","\n","# Cargar el modelo ONNX usando onnxruntime\n","session = ort.InferenceSession(onnx_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Minicky.jpeg'  # Cambia esto a la ruta real de tu imagen\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Redimensionar la imagen al tamaño adecuado para CIFAR-10 (32x32)\n","image = image.resize((32, 32))\n","\n","# Convertir la imagen a un array numpy y escalar los valores de píxeles\n","image_np = np.array(image).astype(np.float32) / 255.0\n","\n","# Cambiar el formato de la imagen a (1, 3, 32, 32) -> (batch_size, canales, alto, ancho)\n","image_np = np.transpose(image_np, (2, 0, 1))  # Cambiar los ejes para que esté en formato canales primero\n","image_np = np.expand_dims(image_np, axis=0)   # Añadir la dimensión de batch_size\n","\n","# Realizar la predicción con el modelo ONNX\n","input_name = session.get_inputs()[0].name  # Obtener el nombre del primer input del modelo\n","output_name = session.get_outputs()[0].name  # Obtener el nombre del primer output del modelo\n","results = session.run([output_name], {input_name: image_np})\n","\n","# Procesar los resultados de la predicción\n","predicciones = results[0][0]  # Acceder a las predicciones de la primera imagen\n","clase_detectada = np.argmax(predicciones)  # Obtener la clase con mayor probabilidad\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NeoHLP3owrMr","executionInfo":{"status":"ok","timestamp":1728419262705,"user_tz":360,"elapsed":237,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"bc8700fa-9e2d-4dc0-c9af-7b18c75e1f2b"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Primera opción detectada: bird con probabilidad 0.30\n","airplane: 0.1740\n","automobile: 0.0070\n","bird: 0.2997\n","cat: 0.0620\n","deer: 0.0268\n","dog: 0.0047\n","frog: 0.2876\n","horse: 0.0033\n","ship: 0.1289\n","truck: 0.0060\n"]}]},{"cell_type":"code","source":["import onnxruntime as ort\n","from PIL import Image\n","import numpy as np\n","import os\n","\n","# Ruta del modelo ONNX exportado\n","onnx_model_path = r'/content/runs/classify/train/weights/best.onnx'\n","\n","# Cargar el modelo ONNX usando onnxruntime\n","session = ort.InferenceSession(onnx_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/gato3.JPG'  # Cambia esto a la ruta real de tu imagen\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Redimensionar la imagen al tamaño adecuado para CIFAR-10 (32x32)\n","image = image.resize((32, 32))\n","\n","# Convertir la imagen a un array numpy y escalar los valores de píxeles\n","image_np = np.array(image).astype(np.float32) / 255.0\n","\n","# Cambiar el formato de la imagen a (1, 3, 32, 32) -> (batch_size, canales, alto, ancho)\n","image_np = np.transpose(image_np, (2, 0, 1))  # Cambiar los ejes para que esté en formato canales primero\n","image_np = np.expand_dims(image_np, axis=0)   # Añadir la dimensión de batch_size\n","\n","# Realizar la predicción con el modelo ONNX\n","input_name = session.get_inputs()[0].name  # Obtener el nombre del primer input del modelo\n","output_name = session.get_outputs()[0].name  # Obtener el nombre del primer output del modelo\n","results = session.run([output_name], {input_name: image_np})\n","\n","# Procesar los resultados de la predicción\n","predicciones = results[0][0]  # Acceder a las predicciones de la primera imagen\n","clase_detectada = np.argmax(predicciones)  # Obtener la clase con mayor probabilidad\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ViVKQzOJv08","executionInfo":{"status":"ok","timestamp":1728419262705,"user_tz":360,"elapsed":16,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"41c67f91-0077-4336-d553-c37aca7063fb"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Primera opción detectada: cat con probabilidad 0.33\n","airplane: 0.0505\n","automobile: 0.0019\n","bird: 0.1422\n","cat: 0.3304\n","deer: 0.0609\n","dog: 0.2337\n","frog: 0.0155\n","horse: 0.1003\n","ship: 0.0500\n","truck: 0.0147\n"]}]},{"cell_type":"code","source":["# Crear modelo en TorchScript\n","\n","model = YOLO(r'/content/runs/classify/train/weights/best.pt')  # load a custom trained model\n","\n","# Export the model\n","model.export(format='torchscript', opset=12, simplify=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261},"id":"rgYJlQunwr8J","executionInfo":{"status":"ok","timestamp":1728419263772,"user_tz":360,"elapsed":1078,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"a920c110-8c45-4f74-e96d-0af0147154e4"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.8 🚀 Python-3.10.12 torch-2.4.1+cu121 CPU (Intel Xeon 2.00GHz)\n","YOLOv8n-cls summary (fused): 73 layers, 1,447,690 parameters, 0 gradients, 3.3 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/runs/classify/train/weights/best.pt' with input shape (1, 3, 32, 32) BCHW and output shape(s) (1, 10) (2.8 MB)\n","\n","\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.4.1+cu121...\n","\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ✅ 0.9s, saved as '/content/runs/classify/train/weights/best.torchscript' (5.7 MB)\n","\n","Export complete (1.0s)\n","Results saved to \u001b[1m/content/runs/classify/train/weights\u001b[0m\n","Predict:         yolo predict task=classify model=/content/runs/classify/train/weights/best.torchscript imgsz=32  \n","Validate:        yolo val task=classify model=/content/runs/classify/train/weights/best.torchscript imgsz=32 data=/content/dataset  \n","Visualize:       https://netron.app\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/runs/classify/train/weights/best.torchscript'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Minicky.jpeg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpwwbjFnwuMT","executionInfo":{"status":"ok","timestamp":1728419264055,"user_tz":360,"elapsed":292,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"25e14278-c1c0-461e-a651-297ff9060bfa"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [    0.26735   0.0056162    0.069619     0.16993     0.29869     0.11881    0.028526    0.033043   0.0040813    0.004324]\n","Primera opción detectada: deer con probabilidad 0.30\n","airplane: 0.2674\n","automobile: 0.0056\n","bird: 0.0696\n","cat: 0.1699\n","deer: 0.2987\n","dog: 0.1188\n","frog: 0.0285\n","horse: 0.0330\n","ship: 0.0041\n","truck: 0.0043\n"]}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/rana.jpg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6U_n1pew0PG","executionInfo":{"status":"ok","timestamp":1728419264363,"user_tz":360,"elapsed":312,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"ec376043-cd1a-47ae-9309-d2b3f4e0471e"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [   0.017408  0.00096919    0.018358     0.17066   0.0038395     0.74885   0.0028839     0.02888   0.0017878   0.0063619]\n","Primera opción detectada: dog con probabilidad 0.75\n","airplane: 0.0174\n","automobile: 0.0010\n","bird: 0.0184\n","cat: 0.1707\n","deer: 0.0038\n","dog: 0.7489\n","frog: 0.0029\n","horse: 0.0289\n","ship: 0.0018\n","truck: 0.0064\n"]}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/cauto_test.jpg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyHmh1onw27m","executionInfo":{"status":"ok","timestamp":1728419264820,"user_tz":360,"elapsed":466,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"d3fb81c6-b2de-414b-d412-b296f5c42471"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [    0.11445  0.00071951    0.042313     0.32875    0.024934     0.45867    0.011296    0.016748    0.001351  0.00077368]\n","Primera opción detectada: dog con probabilidad 0.46\n","airplane: 0.1144\n","automobile: 0.0007\n","bird: 0.0423\n","cat: 0.3287\n","deer: 0.0249\n","dog: 0.4587\n","frog: 0.0113\n","horse: 0.0167\n","ship: 0.0014\n","truck: 0.0008\n"]}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Pelusa.jpeg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K2nacmrcw5SO","executionInfo":{"status":"ok","timestamp":1728419265035,"user_tz":360,"elapsed":221,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"70fad25e-d02c-4948-95a1-8e20a11c1bb8"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [     0.1193   0.0075717     0.12328    0.073613    0.037435    0.086964   0.0057312     0.49494    0.026558    0.024616]\n","Primera opción detectada: horse con probabilidad 0.49\n","airplane: 0.1193\n","automobile: 0.0076\n","bird: 0.1233\n","cat: 0.0736\n","deer: 0.0374\n","dog: 0.0870\n","frog: 0.0057\n","horse: 0.4949\n","ship: 0.0266\n","truck: 0.0246\n"]}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/cauto_test.jpg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728419265273,"user_tz":360,"elapsed":246,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"fb7b097c-f187-4dd7-8b73-838eb03a6da8","id":"uYZzirrCMryv"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [    0.11445  0.00071951    0.042313     0.32875    0.024934     0.45867    0.011296    0.016748    0.001351  0.00077368]\n","Primera opción detectada: dog con probabilidad 0.46\n","airplane: 0.1144\n","automobile: 0.0007\n","bird: 0.0423\n","cat: 0.3287\n","deer: 0.0249\n","dog: 0.4587\n","frog: 0.0113\n","horse: 0.0167\n","ship: 0.0014\n","truck: 0.0008\n"]}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Pelusa_zoom.png'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwJYkTxZw7tM","executionInfo":{"status":"ok","timestamp":1728419265454,"user_tz":360,"elapsed":185,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"c26290de-82e3-4752-ac70-94b7b1028724"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [    0.06415   0.0023195    0.060679     0.54306    0.014297     0.12863     0.13028    0.033392    0.014235   0.0089622]\n","Primera opción detectada: cat con probabilidad 0.54\n","airplane: 0.0642\n","automobile: 0.0023\n","bird: 0.0607\n","cat: 0.5431\n","deer: 0.0143\n","dog: 0.1286\n","frog: 0.1303\n","horse: 0.0334\n","ship: 0.0142\n","truck: 0.0090\n"]}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/gato3.JPG'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1RVEs1i5w-Jf","executionInfo":{"status":"ok","timestamp":1728419265714,"user_tz":360,"elapsed":263,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"a38b4f46-7ac6-403c-f22f-e864b54cf67b"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [    0.33256   0.0034248     0.32347     0.16751    0.045631    0.077772   0.0095668    0.021119    0.013528   0.0054146]\n","Primera opción detectada: airplane con probabilidad 0.33\n","airplane: 0.3326\n","automobile: 0.0034\n","bird: 0.3235\n","cat: 0.1675\n","deer: 0.0456\n","dog: 0.0778\n","frog: 0.0096\n","horse: 0.0211\n","ship: 0.0135\n","truck: 0.0054\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPT7O0HiO+w7xzHc0hxmfjl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}