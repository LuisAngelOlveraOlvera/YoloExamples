{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":923},"id":"THyMZyH4SZ8a","executionInfo":{"status":"ok","timestamp":1727730652141,"user_tz":360,"elapsed":80610,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"5571566c-88c8-4420-c2ff-fc570c0b7423"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-a3bf84c1-c7ff-4412-864f-deac826ff754\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-a3bf84c1-c7ff-4412-864f-deac826ff754\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n","Downloading cifar-10.zip to /content\n","100% 715M/715M [00:07<00:00, 55.1MB/s]\n","100% 715M/715M [00:07<00:00, 95.3MB/s]\n","Archive:  cifar-10.zip\n","  inflating: ./cifar-10/sampleSubmission.csv  \n","  inflating: ./cifar-10/test.7z      \n","  inflating: ./cifar-10/train.7z     \n","  inflating: ./cifar-10/trainLabels.csv  \n","Collecting py7zr\n","  Downloading py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\n","Collecting texttable (from py7zr)\n","  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n","Collecting pycryptodomex>=3.16.0 (from py7zr)\n","  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Collecting pyzstd>=0.15.9 (from py7zr)\n","  Downloading pyzstd-0.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n","Collecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n","  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n","Collecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n","  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n","Collecting multivolumefile>=0.2.3 (from py7zr)\n","  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n","Collecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n","  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n","Collecting brotli>=1.1.0 (from py7zr)\n","  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from py7zr) (5.9.5)\n","Downloading py7zr-0.22.0-py3-none-any.whl (67 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n","Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyzstd-0.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.8/413.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n","Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\n","Successfully installed brotli-1.1.0 inflate64-1.0.0 multivolumefile-0.2.3 py7zr-0.22.0 pybcj-1.0.2 pycryptodomex-3.20.0 pyppmd-1.1.0 pyzstd-0.16.1 texttable-1.7.0\n"]}],"source":["# 1. Instalar y cargar Kaggle, descomprimir el dataset\n","!pip install -q kaggle\n","from google.colab import files\n","\n","# Subir kaggle.json con tus credenciales\n","uploaded = files.upload()\n","\n","# Crear el directorio .kaggle y mover kaggle.json\n","!mkdir -p ~/.kaggle\n","!cp \"{list(uploaded.keys())[0]}\" ~/.kaggle/kaggle.json\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","# Descargar y descomprimir el dataset CIFAR-10\n","!kaggle competitions download -c cifar-10\n","!unzip cifar-10.zip -d ./cifar-10\n","\n","# Instalar librería para descomprimir .7z\n","!pip install py7zr\n","import py7zr\n","\n","# Descomprimir las imágenes de entrenamiento\n","with py7zr.SevenZipFile('/content/cifar-10/train.7z', mode='r') as z:\n","    z.extractall(path='/content/cifar-10/train')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"veITmubYekQ_","colab":{"base_uri":"https://localhost:8080/","height":284},"executionInfo":{"status":"ok","timestamp":1727730653283,"user_tz":360,"elapsed":1151,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"3cc07eac-0d00-4502-f9a5-b2e587d48a29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Imagen: 9580.png, Tamaño: 32x32\n","Imagen: 37019.png, Tamaño: 32x32\n","Imagen: 42143.png, Tamaño: 32x32\n","Imagen: 6580.png, Tamaño: 32x32\n","Imagen: 23896.png, Tamaño: 32x32\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 5 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# 2. Hacer un muestreo del dataset para verificar su contenido y tamaño\n","import os\n","import random\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# Directorio de las imágenes\n","train_dir = '/content/cifar-10/train/train'\n","\n","# Obtener una lista de archivos de imagen\n","image_files = [f for f in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, f))]\n","\n","# Seleccionar 5 imágenes aleatoriamente\n","random_images = random.sample(image_files, 5)\n","\n","# Mostrar las imágenes y sus dimensiones\n","plt.figure(figsize=(10,5))\n","for i, image_file in enumerate(random_images):\n","    img = Image.open(os.path.join(train_dir, image_file))\n","    width, height = img.size\n","    print(f\"Imagen: {image_file}, Tamaño: {width}x{height}\")\n","    plt.subplot(1, 5, i + 1)\n","    plt.imshow(img)\n","    plt.axis('off')\n","    plt.title(image_file)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WhxWI75NgwaV"},"source":["# **Clases de CIFAR-10**\n","airplane\n","automobile\n","bird\n","cat\n","deer\n","dog\n","frog\n","horse\n","ship\n","truck\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"g7mJ78XCgu3q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727730653284,"user_tz":360,"elapsed":22,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"e59f68bc-33b8-47ca-cbb4-35781e47ff93"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 50000\n"]}],"source":["# 3. Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/cifar-10/train/train'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"d2tVT9V1iFnl","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1727730653818,"user_tz":360,"elapsed":549,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"6cab097c-44d7-4ba5-ecb7-ee490c5881be"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id       label\n","0   1        frog\n","1   2       truck\n","2   3       truck\n","3   4        deer\n","4   5  automobile"],"text/html":["\n","  <div id=\"df-ebd3af43-d556-4c6a-a639-05535ee5775a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>frog</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>truck</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>truck</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>deer</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>automobile</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ebd3af43-d556-4c6a-a639-05535ee5775a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ebd3af43-d556-4c6a-a639-05535ee5775a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ebd3af43-d556-4c6a-a639-05535ee5775a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-002642cb-b7ac-40f8-8c9b-3d61198fed17\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-002642cb-b7ac-40f8-8c9b-3d61198fed17')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-002642cb-b7ac-40f8-8c9b-3d61198fed17 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"labels_df","summary":"{\n  \"name\": \"labels_df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14433,\n        \"min\": 1,\n        \"max\": 50000,\n        \"num_unique_values\": 50000,\n        \"samples\": [\n          33554,\n          9428,\n          200\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"dog\",\n          \"truck\",\n          \"horse\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}],"source":["# 4. Análisis rápido de las etiquetas de trainLabels.csv\n","import pandas as pd\n","\n","# Cargar el CSV\n","labels_df = pd.read_csv('/content/cifar-10/trainLabels.csv')\n","\n","labels_df.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"a4-hz4KfSTfb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727730653818,"user_tz":360,"elapsed":26,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"8719d53b-3f01-4c79-885d-977f5ef8fbfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["100000\n"]}],"source":["print(labels_df.size)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pN3P1Uh3ifyy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727730658631,"user_tz":360,"elapsed":4820,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"5495c1b3-b343-4025-b72b-125e1dd401a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número de imágenes movidas a entrenamiento: 40000\n","Número de imágenes movidas a validación: 10000\n"]}],"source":["# 3. Crear listas de imágenes y etiquetas\n","from sklearn.model_selection import train_test_split\n","import shutil\n","import os\n","\n","# Asegurarse de que el ID esté correctamente formateado para coincidir con los nombres de archivo\n","\n","images = labels_df['id'].apply(lambda x: f'{x}.png').values\n","labels = labels_df['label'].values\n","\n","# Dividir en entrenamientos y validación\n","train_imgs, val_imgs, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n","\n","# Crear las carpetas de entrenamiento y validación\n","train_path = '/content/dataset/train'\n","val_path = '/content/dataset/val'\n","\n","# Crear caroetas por clase dentro de val y train\n","for label in labels_df[\"label\"].unique():\n","  os.makedirs(os.path.join(train_path, label), exist_ok=True)\n","  os.makedirs(os.path.join(val_path, label), exist_ok=True)\n","\n","train_count = 0\n","val_count = 0\n","\n","# Mover las imágenes de entrenamiento\n","for img, label in zip(train_imgs, train_labels):\n","    img_src = f'/content/cifar-10/train/train/{img}'\n","    img_dst = os.path.join(train_path, str(label), img)\n","    if os.path.exists(img_src):  # Verificar si la imagen existe\n","        shutil.move(img_src, img_dst)\n","        train_count += 1\n","\n","print(f'Número de imágenes movidas a entrenamiento: {train_count}')\n","\n","# Mover las imágenes de validación\n","for img, label in zip(val_imgs, val_labels):\n","    img_src = f'/content/cifar-10/train/train/{img}'\n","    img_dst = os.path.join(val_path, str(label), img)\n","    if os.path.exists(img_src):  # Verificar si la imagen existe\n","        shutil.move(img_src, img_dst)\n","        val_count += 1\n","\n","print(f'Número de imágenes movidas a validación: {val_count}')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"zaqjvfCHToSI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727730658631,"user_tz":360,"elapsed":13,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"471e36c6-cffe-422a-d06d-9356aae8dcd5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 4000\n"]}],"source":["# 3.1 Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/train/airplane'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"FJ1Bz5lYTuoY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727730658826,"user_tz":360,"elapsed":204,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"3f966ccb-69e7-48a5-aa30-92e80f28253b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 4000\n"]}],"source":["# 3.2 Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/train/cat'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"HJDvAF8HTyt9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727730658826,"user_tz":360,"elapsed":15,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"81c9e264-9901-46a0-c952-abbdcc8fff3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 1000\n"]}],"source":["# 3.3 Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/val/deer'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"UVDUfDSaibek","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727730658827,"user_tz":360,"elapsed":12,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"8f13a36b-57c2-4731-c45f-ebbb9d8800bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["La carpeta no está vacía para train.\n","La carpeta no está vacía para val.\n"]}],"source":["# 6. Verificar que las imágenes se han movido de forma exitosa a las carpetas\n","import os\n","\n","def is_directory_empty(directory):\n","  return not os.listdir(directory)\n","\n","directory_train =  \"/content/dataset/train/airplane\"\n","\n","if is_directory_empty(directory_train):\n","  print(\"La carpeta está vacía para train.\")\n","else:\n","  print(\"La carpeta no está vacía para train.\")\n","\n","directory_val =  \"/content/dataset/val/airplane\"\n","\n","if is_directory_empty(directory_train):\n","  print(\"La carpeta está vacía para val.\")\n","else:\n","  print(\"La carpeta no está vacía para val.\")"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ClxtGVQKRQEt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727730659016,"user_tz":360,"elapsed":196,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"66d609cc-3b60-4a0c-f415-d4bc574a8705"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 4000\n"]}],"source":["# 3.1. Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/train/airplane'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"9GcZYwJ9Raff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727730659017,"user_tz":360,"elapsed":9,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"c0400d2e-7281-42a4-f611-55c8e7efece2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 4000\n"]}],"source":["# 3.2 Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/train/automobile'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"GNdhtWNjRhdu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727730659209,"user_tz":360,"elapsed":196,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"a560077f-b040-41d4-aad3-0e8732f955c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 4000\n"]}],"source":["# 3.2 Tamaño de elementos para carpeta train\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/train/bird'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"2VxfTVEeRtTl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727730659210,"user_tz":360,"elapsed":9,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"1be94290-4ea2-46d0-f9a6-e6879b3d109f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de elementos: 1000\n"]}],"source":["# 3.1. Tamaño de elementos para carpeta VAL\n","import os\n","\n","# ruta de la carpeta\n","dir_path = r'/content/dataset/val/airplane'\n","count = 0\n","# Itera dentro de la carpeta, es un contador\n","for path in os.listdir(dir_path):\n","    # verifica si la ruta actual es un arhico\n","    if os.path.isfile(os.path.join(dir_path, path)):\n","        count += 1\n","print('Número total de elementos:', count)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"b-IdX0v4lxjG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727735251678,"user_tz":360,"elapsed":4592473,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"97ab92f1-96e6-4dcb-cf2b-098ee1611dd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.1-py3-none-any.whl.metadata (34 kB)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.8-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Downloading ultralytics-8.3.1-py3-none-any.whl (881 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.3/881.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.8-py3-none-any.whl (26 kB)\n","Installing collected packages: ultralytics-thop, ultralytics\n","Successfully installed ultralytics-8.3.1 ultralytics-thop-2.0.8\n","Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-cls.pt to 'yolov8n-cls.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5.31M/5.31M [00:00<00:00, 42.8MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.1 🚀 Python-3.10.12 torch-2.4.1+cu121 CPU (Intel Xeon 2.20GHz)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/content/dataset, epochs=20, time=None, patience=100, batch=16, imgsz=32, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/dataset/train... found 40000 images in 10 classes ✅ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/dataset/val... found 10000 images in 10 classes ✅ \n","\u001b[34m\u001b[1mtest:\u001b[0m None...\n","Overriding model.yaml nc=1000 with nc=10\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    343050  ultralytics.nn.modules.head.Classify         [256, 10]                     \n","YOLOv8n-cls summary: 99 layers, 1,451,098 parameters, 1,451,098 gradients, 3.4 GFLOPs\n","Transferred 156/158 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset/train... 40000 images, 0 corrupt: 100%|██████████| 40000/40000 [00:09<00:00, 4331.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/dataset/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/val... 10000 images, 0 corrupt: 100%|██████████| 10000/10000 [00:02<00:00, 4249.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/dataset/val.cache\n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n","Image sizes 32 train, 32 val\n","Using 0 dataloader workers\n","Logging results to \u001b[1mruns/classify/train\u001b[0m\n","Starting training for 20 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       1/20         0G      2.524         16         32:   0%|          | 6/2500 [00:00<04:40,  8.90it/s]"]},{"output_type":"stream","name":"stdout","text":["Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"output_type":"stream","name":"stderr","text":["       1/20         0G       2.67         16         32:   1%|          | 13/2500 [00:01<03:57, 10.48it/s]\n","100%|██████████| 755k/755k [00:00<00:00, 11.8MB/s]\n","       1/20         0G      1.961         16         32: 100%|██████████| 2500/2500 [03:54<00:00, 10.68it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.61it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.496      0.926\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       2/20         0G       1.83         16         32: 100%|██████████| 2500/2500 [03:39<00:00, 11.37it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.494      0.922\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       3/20         0G      1.827         16         32: 100%|██████████| 2500/2500 [03:36<00:00, 11.57it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 26.00it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.548      0.932\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       4/20         0G      1.591         16         32: 100%|██████████| 2500/2500 [03:30<00:00, 11.86it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.25it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.635      0.959\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       5/20         0G      1.424         16         32: 100%|██████████| 2500/2500 [03:34<00:00, 11.68it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.20it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.673      0.971\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       6/20         0G      1.338         16         32: 100%|██████████| 2500/2500 [03:34<00:00, 11.64it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.35it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.689      0.972\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       7/20         0G      1.277         16         32: 100%|██████████| 2500/2500 [03:33<00:00, 11.69it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:14<00:00, 21.49it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.703      0.975\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       8/20         0G      1.241         16         32: 100%|██████████| 2500/2500 [03:34<00:00, 11.66it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.65it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.715      0.979\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       9/20         0G       1.19         16         32: 100%|██████████| 2500/2500 [03:30<00:00, 11.85it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.47it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.726      0.978\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      10/20         0G      1.169         16         32: 100%|██████████| 2500/2500 [03:31<00:00, 11.81it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 24.85it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.733       0.98\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      11/20         0G      1.156         16         32: 100%|██████████| 2500/2500 [03:31<00:00, 11.80it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.46it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.738      0.981\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      12/20         0G      1.114         16         32: 100%|██████████| 2500/2500 [03:31<00:00, 11.85it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.14it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.741      0.984\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      13/20         0G      1.091         16         32: 100%|██████████| 2500/2500 [03:32<00:00, 11.76it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:11<00:00, 26.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.755      0.984\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      14/20         0G       1.06         16         32: 100%|██████████| 2500/2500 [03:33<00:00, 11.71it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.25it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.759      0.985\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      15/20         0G      1.032         16         32: 100%|██████████| 2500/2500 [03:33<00:00, 11.71it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 24.80it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.763      0.985\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      16/20         0G      1.007         16         32: 100%|██████████| 2500/2500 [03:31<00:00, 11.80it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.58it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.767      0.986\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      17/20         0G     0.9852         16         32: 100%|██████████| 2500/2500 [03:29<00:00, 11.94it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all       0.77      0.986\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      18/20         0G     0.9512         16         32: 100%|██████████| 2500/2500 [03:31<00:00, 11.85it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 24.50it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.766      0.987\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      19/20         0G     0.9441         16         32: 100%|██████████| 2500/2500 [03:30<00:00, 11.89it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.769      0.987\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      20/20         0G     0.9252         16         32: 100%|██████████| 2500/2500 [03:30<00:00, 11.90it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:12<00:00, 25.93it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.762      0.987\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","20 epochs completed in 1.260 hours.\n","Optimizer stripped from runs/classify/train/weights/last.pt, 3.0MB\n","Optimizer stripped from runs/classify/train/weights/best.pt, 3.0MB\n","\n","Validating runs/classify/train/weights/best.pt...\n","Ultralytics 8.3.1 🚀 Python-3.10.12 torch-2.4.1+cu121 CPU (Intel Xeon 2.20GHz)\n","YOLOv8n-cls summary (fused): 73 layers, 1,447,690 parameters, 0 gradients, 3.3 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/dataset/train... found 40000 images in 10 classes ✅ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/dataset/val... found 10000 images in 10 classes ✅ \n","\u001b[34m\u001b[1mtest:\u001b[0m None...\n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|██████████| 313/313 [00:11<00:00, 28.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.769      0.987\n","Speed: 0.0ms preprocess, 0.6ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/train\u001b[0m\n","Results saved to \u001b[1mruns/classify/train\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n","\n","confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7df92e1dd9c0>\n","curves: []\n","curves_results: []\n","fitness: 0.8781999945640564\n","keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n","results_dict: {'metrics/accuracy_top1': 0.7689999938011169, 'metrics/accuracy_top5': 0.9873999953269958, 'fitness': 0.8781999945640564}\n","save_dir: PosixPath('runs/classify/train')\n","speed: {'preprocess': 0.000546574592590332, 'inference': 0.5518328905105591, 'loss': 7.674694061279298e-05, 'postprocess': 5.98907470703125e-05}\n","task: 'classify'\n","top1: 0.7689999938011169\n","top5: 0.9873999953269958"]},"metadata":{},"execution_count":15}],"source":["### ENTRENAMIENTO DEL MODELO ####\n","# 7. Entrenar el modelo\n","!pip install ultralytics\n","from ultralytics import YOLO\n","\n","# Cargar el modelo yolov8n-cls.pt para clasificación\n","model = YOLO('yolov8n-cls.pt')\n","# Entrenar el modelo con CIFAR-10\n","model.train(data='/content/dataset', epochs=20, imgsz=32)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"qGusSGqxnA-w","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1T4N-YJHjjodIXfhAonhfppH3LnYK5zOh"},"executionInfo":{"status":"ok","timestamp":1727735261731,"user_tz":360,"elapsed":10067,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"07db1cef-fa6d-4d89-96c7-dff84078d88d"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# 8. Visualizar los resultados de las gráficas YOLO\n","import os\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# Ruta del directorio donde se encuentran las imágenes\n","directory = \"/content/runs/classify/train\"\n","\n","# Listar todos los archivos en el directorio\n","files = os.listdir(directory)\n","print(\"Estos son los archivos de resultado de entrenamiento\", files)\n","\n","# Filtrar solo los archivos de imagen (extensiones comunes)\n","image_files = [file for file in files if file.endswith(('.png', '.jpg', '.jpeg'))]\n","\n","# Mostrar todas las imágenes\n","# Mostrar todas las imágenes\n","if image_files:\n","    for image_file in image_files:\n","        image_path = os.path.join(directory, image_file)\n","\n","        # Cargar la imagen\n","        image = Image.open(image_path)\n","\n","        # Mostrar la imagen\n","        plt.figure()\n","        plt.imshow(image)\n","        plt.axis('off')  # Ocultar los ejes\n","        plt.title(image_file)  # Título con el nombre del archivo\n","    plt.show()\n","else:\n","    print(\"No se encontraron imágenes en la carpeta.\")"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"kZYgY-H7pMjn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727735261937,"user_tz":360,"elapsed":232,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"cb5ff18b-d9b3-412d-8e5a-77b1489f5d0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 airplane 0.76, ship 0.19, bird 0.02, deer 0.01, truck 0.01, 10.3ms\n","Speed: 14.0ms preprocess, 10.3ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train2\u001b[0m\n"]}],"source":["# 9. Predicción con imagen del dataset\n","import os\n","from PIL import Image\n","\n","directory = \"/content/dataset/train/airplane\"\n","\n","# Si la carpeta tiene imágenes, carga la primera imagen\n","image_path = os.path.join(directory, os.listdir(directory)[0])\n","image = Image.open(image_path)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"Dl4Zc03253sg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727735263185,"user_tz":360,"elapsed":1264,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"9d853cdb-3710-474b-f9d2-9c2bc7692a3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-09-30 22:27:41--  https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/rana.jpg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 107572 (105K) [image/jpeg]\n","Saving to: ‘rana.jpg’\n","\n","rana.jpg            100%[===================>] 105.05K  --.-KB/s    in 0.03s   \n","\n","2024-09-30 22:27:42 (3.10 MB/s) - ‘rana.jpg’ saved [107572/107572]\n","\n","\n","0: 32x32 frog 0.82, bird 0.14, deer 0.01, airplane 0.01, cat 0.01, 5.1ms\n","Speed: 16.9ms preprocess, 5.1ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train3\u001b[0m\n"]}],"source":["!wget https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/rana.jpg\n","\n","# 10.1  Predicción con imagen de internet RANA\n","import os\n","from PIL import Image\n","\n","directory = \"/content/rana.jpg\"\n","\n","image = Image.open(directory)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/cauto_test.jpg\n","\n","# 10.1  Predicción con imagen de internet RANA\n","import os\n","from PIL import Image\n","\n","directory = \"/content/cauto_test.jpg\"\n","\n","image = Image.open(directory)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FCEbWO_mt05p","executionInfo":{"status":"ok","timestamp":1727735569386,"user_tz":360,"elapsed":1600,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"96bb6fd0-0ac9-47dd-d206-4570442e5f3e"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-09-30 22:32:47--  https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/cauto_test.jpg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 350294 (342K) [image/jpeg]\n","Saving to: ‘cauto_test.jpg.1’\n","\n","cauto_test.jpg.1    100%[===================>] 342.08K  --.-KB/s    in 0.06s   \n","\n","2024-09-30 22:32:48 (5.70 MB/s) - ‘cauto_test.jpg.1’ saved [350294/350294]\n","\n","\n","0: 32x32 automobile 0.86, frog 0.05, airplane 0.04, truck 0.02, ship 0.01, 14.8ms\n","Speed: 14.0ms preprocess, 14.8ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train4\u001b[0m\n"]}]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/Pelusa.jpeg\n","\n","# 10.1  Predicción con imagen de internet RANA\n","import os\n","from PIL import Image\n","\n","directory = \"/content/Pelusa.jpeg\"\n","\n","image = Image.open(directory)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTrPFWrgt1EX","executionInfo":{"status":"ok","timestamp":1727735675325,"user_tz":360,"elapsed":1333,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"ed752536-49ec-463a-b8e9-2304a66b37e8"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-09-30 22:34:33--  https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/Pelusa.jpeg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 239195 (234K) [image/jpeg]\n","Saving to: ‘Pelusa.jpeg’\n","\n","Pelusa.jpeg         100%[===================>] 233.59K  --.-KB/s    in 0.05s   \n","\n","2024-09-30 22:34:34 (4.47 MB/s) - ‘Pelusa.jpeg’ saved [239195/239195]\n","\n","\n","0: 32x32 horse 0.52, cat 0.17, dog 0.13, deer 0.13, truck 0.02, 4.1ms\n","Speed: 18.1ms preprocess, 4.1ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train5\u001b[0m\n"]}]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/gato3.JPG\n","\n","# 10.1  Predicción con imagen de internet RANA\n","import os\n","from PIL import Image\n","\n","directory = \"/content/gato3.JPG\"\n","\n","image = Image.open(directory)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPvNEfHat1JF","executionInfo":{"status":"ok","timestamp":1727735749972,"user_tz":360,"elapsed":1247,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"2279e600-9b67-4859-edfc-d270070d31b0"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-09-30 22:35:48--  https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/gato3.JPG\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 88203 (86K) [image/jpeg]\n","Saving to: ‘gato3.JPG’\n","\n","\rgato3.JPG             0%[                    ]       0  --.-KB/s               \rgato3.JPG           100%[===================>]  86.14K  --.-KB/s    in 0.03s   \n","\n","2024-09-30 22:35:49 (2.52 MB/s) - ‘gato3.JPG’ saved [88203/88203]\n","\n","\n","0: 32x32 cat 0.46, dog 0.40, ship 0.04, horse 0.03, bird 0.03, 9.9ms\n","Speed: 16.0ms preprocess, 9.9ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train6\u001b[0m\n"]}]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/Minicky.jpeg\n","\n","# 10.1  Predicción con imagen de internet RANA\n","import os\n","from PIL import Image\n","\n","directory = \"/content/Minicky.jpeg\"\n","\n","image = Image.open(directory)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZ4Df2qht1Mb","executionInfo":{"status":"ok","timestamp":1727735805452,"user_tz":360,"elapsed":1854,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"f68bf4e6-02f7-40be-a68e-9a61e2179f58"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-09-30 22:36:43--  https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/Minicky.jpeg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 213012 (208K) [image/jpeg]\n","Saving to: ‘Minicky.jpeg’\n","\n","Minicky.jpeg        100%[===================>] 208.02K  --.-KB/s    in 0.05s   \n","\n","2024-09-30 22:36:43 (4.02 MB/s) - ‘Minicky.jpeg’ saved [213012/213012]\n","\n","\n","0: 32x32 cat 0.54, ship 0.22, frog 0.08, bird 0.05, airplane 0.04, 10.3ms\n","Speed: 26.7ms preprocess, 10.3ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train7\u001b[0m\n"]}]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/Pelusa_zoom.png\n","\n","# 10.1  Predicción con imagen de internet RANA\n","import os\n","from PIL import Image\n","\n","directory = \"/content/Pelusa_zoom.png\"\n","\n","image = Image.open(directory)\n","\n","# Realizar la predicción con el modelo YOLO\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones\n","image.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYkCudbit1Po","executionInfo":{"status":"ok","timestamp":1727735849945,"user_tz":360,"elapsed":1025,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"518758d3-2277-43f7-d848-75aee02151d4"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-09-30 22:37:28--  https://raw.githubusercontent.com/LuisAngelOlveraOlvera/YoloExamples/main/ImageTestYolo/Pelusa_zoom.png\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 443434 (433K) [image/png]\n","Saving to: ‘Pelusa_zoom.png’\n","\n","Pelusa_zoom.png     100%[===================>] 433.04K  --.-KB/s    in 0.06s   \n","\n","2024-09-30 22:37:29 (7.01 MB/s) - ‘Pelusa_zoom.png’ saved [443434/443434]\n","\n","\n","0: 32x32 cat 0.39, ship 0.23, frog 0.19, truck 0.09, automobile 0.03, 11.4ms\n","Speed: 4.0ms preprocess, 11.4ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/train8\u001b[0m\n"]}]},{"cell_type":"code","execution_count":27,"metadata":{"id":"sIliKVVusyBW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727735945677,"user_tz":360,"elapsed":23367,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"2b1bcd24-65d7-462a-d501-febf43138880"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.1 🚀 Python-3.10.12 torch-2.4.1+cu121 CPU (Intel Xeon 2.20GHz)\n","YOLOv8n-cls summary (fused): 73 layers, 1,447,690 parameters, 0 gradients, 3.3 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/dataset/train... found 40000 images in 10 classes ✅ \n","\u001b[34m\u001b[1mval:\u001b[0m /content/dataset/val... found 10000 images in 10 classes ✅ \n","\u001b[34m\u001b[1mtest:\u001b[0m None...\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/val... 10000 images, 0 corrupt: 100%|██████████| 10000/10000 [00:00<?, ?it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 625/625 [00:17<00:00, 35.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.769      0.987\n","Speed: 0.0ms preprocess, 1.0ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/val\u001b[0m\n","Top-1 Accuracy: 0.7689999938011169\n","Top-5 Accuracy: 0.9873999953269958\n","Fitness: 0.8781999945640564\n","\n","Todas las métricas:\n","metrics/accuracy_top1: 0.7689999938011169\n","metrics/accuracy_top5: 0.9873999953269958\n","fitness: 0.8781999945640564\n"]}],"source":["# 11. Validar el modelo\n","from ultralytics import YOLO\n","\n","# Cargar el modelo\n","model = YOLO(\"/content/runs/classify/train/weights/best.pt\")\n","\n","# Validar el modelo\n","metrics = model.val()\n","\n","# Acceder a las métricas de clasificación\n","print(f\"Top-1 Accuracy: {metrics.top1}\")\n","print(f\"Top-5 Accuracy: {metrics.top5}\")\n","print(f\"Fitness: {metrics.fitness}\")\n","\n","# Si quieres ver todas las métricas disponibles\n","print(\"\\nTodas las métricas:\")\n","for key, value in metrics.results_dict.items():\n","    print(f\"{key}: {value}\")"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"fiV6d370yipm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727736072572,"user_tz":360,"elapsed":2853,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"5072eb7a-c093-4589-d77e-51ef43c1daeb"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 horse 0.52, cat 0.17, dog 0.13, deer 0.13, truck 0.02, 16.8ms\n","Speed: 54.0ms preprocess, 16.8ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/predict\u001b[0m\n","Clase: horse, Confianza: 0.52\n","airplane: 0.0074\n","automobile: 0.0071\n","bird: 0.0105\n","cat: 0.1721\n","deer: 0.1278\n","dog: 0.1334\n","frog: 0.0027\n","horse: 0.5219\n","ship: 0.0014\n","truck: 0.0158\n"]}],"source":["import os\n","from PIL import Image\n","from ultralytics import YOLO  # Asegúrate de importar YOLO si no lo has hecho ya\n","\n","# Ruta del modelo YOLO\n","model_path = r'/content/runs/classify/train/weights/best.pt'\n","\n","# Cargar el modelo YOLO\n","model = YOLO(model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Pelusa.jpeg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Realizar la predicción con el modelo YOLO usando predict() y un umbral de confianza de 0.25\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones (si se ha guardado o procesado)\n","image.show()\n","\n","# Procesar y mostrar los resultados de la predicción\n","for result in results:\n","    names = result.names  # Nombres de las clases\n","    top_class = result.probs.top1  # Índice de la clase con mayor probabilidad\n","    top_confidence = result.probs.top1conf.item()  # Probabilidad de la clase con mayor probabilidad\n","\n","    print(f\"Clase: {names[top_class]}, Confianza: {top_confidence:.2f}\")\n","\n","    # Si quieres mostrar todas las probabilidades:\n","    for i, prob in enumerate(result.probs.data):\n","        print(f\"{names[i]}: {prob:.4f}\")"]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from ultralytics import YOLO  # Asegúrate de importar YOLO si no lo has hecho ya\n","\n","# Ruta del modelo YOLO\n","model_path = r'/content/runs/classify/train/weights/best.pt'\n","\n","# Cargar el modelo YOLO\n","model = YOLO(model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Pelusa_zoom.png'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Realizar la predicción con el modelo YOLO usando predict() y un umbral de confianza de 0.25\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones (si se ha guardado o procesado)\n","image.show()\n","\n","# Procesar y mostrar los resultados de la predicción\n","for result in results:\n","    names = result.names  # Nombres de las clases\n","    top_class = result.probs.top1  # Índice de la clase con mayor probabilidad\n","    top_confidence = result.probs.top1conf.item()  # Probabilidad de la clase con mayor probabilidad\n","\n","    print(f\"Clase: {names[top_class]}, Confianza: {top_confidence:.2f}\")\n","\n","    # Si quieres mostrar todas las probabilidades:\n","    for i, prob in enumerate(result.probs.data):\n","        print(f\"{names[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwag8JyLv6Ni","executionInfo":{"status":"ok","timestamp":1727736131288,"user_tz":360,"elapsed":613,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"07dad818-f804-4c7b-fa54-3d89977c2d18"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 cat 0.39, ship 0.23, frog 0.19, truck 0.09, automobile 0.03, 7.1ms\n","Speed: 8.2ms preprocess, 7.1ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/predict2\u001b[0m\n","Clase: cat, Confianza: 0.39\n","airplane: 0.0073\n","automobile: 0.0257\n","bird: 0.0222\n","cat: 0.3892\n","deer: 0.0095\n","dog: 0.0192\n","frog: 0.1932\n","horse: 0.0111\n","ship: 0.2289\n","truck: 0.0936\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from ultralytics import YOLO  # Asegúrate de importar YOLO si no lo has hecho ya\n","\n","# Ruta del modelo YOLO\n","model_path = r'/content/runs/classify/train/weights/best.pt'\n","\n","# Cargar el modelo YOLO\n","model = YOLO(model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/cauto_test.jpg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Realizar la predicción con el modelo YOLO usando predict() y un umbral de confianza de 0.25\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones (si se ha guardado o procesado)\n","image.show()\n","\n","# Procesar y mostrar los resultados de la predicción\n","for result in results:\n","    names = result.names  # Nombres de las clases\n","    top_class = result.probs.top1  # Índice de la clase con mayor probabilidad\n","    top_confidence = result.probs.top1conf.item()  # Probabilidad de la clase con mayor probabilidad\n","\n","    print(f\"Clase: {names[top_class]}, Confianza: {top_confidence:.2f}\")\n","\n","    # Si quieres mostrar todas las probabilidades:\n","    for i, prob in enumerate(result.probs.data):\n","        print(f\"{names[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oepeuuHv84B","executionInfo":{"status":"ok","timestamp":1727736189472,"user_tz":360,"elapsed":715,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"558ca4ce-4c6e-458e-dd5b-cd48bc8039e3"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 automobile 0.86, frog 0.05, airplane 0.04, truck 0.02, ship 0.01, 5.6ms\n","Speed: 11.0ms preprocess, 5.6ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/predict3\u001b[0m\n","Clase: automobile, Confianza: 0.86\n","airplane: 0.0368\n","automobile: 0.8617\n","bird: 0.0058\n","cat: 0.0001\n","deer: 0.0077\n","dog: 0.0007\n","frog: 0.0527\n","horse: 0.0005\n","ship: 0.0128\n","truck: 0.0210\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from ultralytics import YOLO  # Asegúrate de importar YOLO si no lo has hecho ya\n","\n","# Ruta del modelo YOLO\n","model_path = r'/content/runs/classify/train/weights/best.pt'\n","\n","# Cargar el modelo YOLO\n","model = YOLO(model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/rana.jpg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Realizar la predicción con el modelo YOLO usando predict() y un umbral de confianza de 0.25\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones (si se ha guardado o procesado)\n","image.show()\n","\n","# Procesar y mostrar los resultados de la predicción\n","for result in results:\n","    names = result.names  # Nombres de las clases\n","    top_class = result.probs.top1  # Índice de la clase con mayor probabilidad\n","    top_confidence = result.probs.top1conf.item()  # Probabilidad de la clase con mayor probabilidad\n","\n","    print(f\"Clase: {names[top_class]}, Confianza: {top_confidence:.2f}\")\n","\n","    # Si quieres mostrar todas las probabilidades:\n","    for i, prob in enumerate(result.probs.data):\n","        print(f\"{names[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ase8qq8tv_-S","executionInfo":{"status":"ok","timestamp":1727736256109,"user_tz":360,"elapsed":1525,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"be8217f3-30d4-4483-b6bb-9efe91e35521"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 frog 0.82, bird 0.14, deer 0.01, airplane 0.01, cat 0.01, 7.3ms\n","Speed: 30.0ms preprocess, 7.3ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/predict4\u001b[0m\n","Clase: frog, Confianza: 0.82\n","airplane: 0.0127\n","automobile: 0.0012\n","bird: 0.1381\n","cat: 0.0060\n","deer: 0.0146\n","dog: 0.0020\n","frog: 0.8218\n","horse: 0.0019\n","ship: 0.0008\n","truck: 0.0011\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from ultralytics import YOLO  # Asegúrate de importar YOLO si no lo has hecho ya\n","\n","# Ruta del modelo YOLO\n","model_path = r'/content/runs/classify/train/weights/best.pt'\n","\n","# Cargar el modelo YOLO\n","model = YOLO(model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/gato3.JPG'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Realizar la predicción con el modelo YOLO usando predict() y un umbral de confianza de 0.25\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones (si se ha guardado o procesado)\n","image.show()\n","\n","# Procesar y mostrar los resultados de la predicción\n","for result in results:\n","    names = result.names  # Nombres de las clases\n","    top_class = result.probs.top1  # Índice de la clase con mayor probabilidad\n","    top_confidence = result.probs.top1conf.item()  # Probabilidad de la clase con mayor probabilidad\n","\n","    print(f\"Clase: {names[top_class]}, Confianza: {top_confidence:.2f}\")\n","\n","    # Si quieres mostrar todas las probabilidades:\n","    for i, prob in enumerate(result.probs.data):\n","        print(f\"{names[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FMAxdNgRwDZD","executionInfo":{"status":"ok","timestamp":1727736617588,"user_tz":360,"elapsed":1480,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"135af5b2-db42-401d-ddba-a9113f17fa06"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 cat 0.46, dog 0.40, ship 0.04, horse 0.03, bird 0.03, 16.5ms\n","Speed: 20.8ms preprocess, 16.5ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/predict5\u001b[0m\n","Clase: cat, Confianza: 0.46\n","airplane: 0.0024\n","automobile: 0.0013\n","bird: 0.0309\n","cat: 0.4612\n","deer: 0.0193\n","dog: 0.4018\n","frog: 0.0056\n","horse: 0.0331\n","ship: 0.0401\n","truck: 0.0044\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from ultralytics import YOLO  # Asegúrate de importar YOLO si no lo has hecho ya\n","\n","# Ruta del modelo YOLO\n","model_path = r'/content/runs/classify/train/weights/best.pt'\n","\n","# Cargar el modelo YOLO\n","model = YOLO(model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Minicky.jpeg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Realizar la predicción con el modelo YOLO usando predict() y un umbral de confianza de 0.25\n","results = model.predict(image, conf=0.25, save=True)\n","\n","# Mostrar la imagen con las predicciones (si se ha guardado o procesado)\n","image.show()\n","\n","# Procesar y mostrar los resultados de la predicción\n","for result in results:\n","    names = result.names  # Nombres de las clases\n","    top_class = result.probs.top1  # Índice de la clase con mayor probabilidad\n","    top_confidence = result.probs.top1conf.item()  # Probabilidad de la clase con mayor probabilidad\n","\n","    print(f\"Clase: {names[top_class]}, Confianza: {top_confidence:.2f}\")\n","\n","    # Si quieres mostrar todas las probabilidades:\n","    for i, prob in enumerate(result.probs.data):\n","        print(f\"{names[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HFPZjEg0wELu","executionInfo":{"status":"ok","timestamp":1727736656201,"user_tz":360,"elapsed":1862,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"9e37e748-5080-45e3-bc4a-da12fe089754"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 32x32 cat 0.54, ship 0.22, frog 0.08, bird 0.05, airplane 0.04, 11.9ms\n","Speed: 35.5ms preprocess, 11.9ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 32)\n","Results saved to \u001b[1mruns/classify/predict6\u001b[0m\n","Clase: cat, Confianza: 0.54\n","airplane: 0.0412\n","automobile: 0.0220\n","bird: 0.0521\n","cat: 0.5372\n","deer: 0.0090\n","dog: 0.0252\n","frog: 0.0829\n","horse: 0.0060\n","ship: 0.2188\n","truck: 0.0057\n"]}]},{"cell_type":"code","source":["# 14. Exportar modelo a onnx\n","from ultralytics import YOLO\n","\n","model = YOLO(r'/content/runs/classify/train/weights/best.pt')  # load a custom trained model\n","\n","# Export the model\n","model.export(format='onnx', opset=12, simplify=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":923},"id":"y8A_mzcAwGmU","executionInfo":{"status":"ok","timestamp":1727736708137,"user_tz":360,"elapsed":11604,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"7bc5d78e-801f-4868-be3a-59fc11d78b3d"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.1 🚀 Python-3.10.12 torch-2.4.1+cu121 CPU (Intel Xeon 2.20GHz)\n","YOLOv8n-cls summary (fused): 73 layers, 1,447,690 parameters, 0 gradients, 3.3 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/runs/classify/train/weights/best.pt' with input shape (1, 3, 32, 32) BCHW and output shape(s) (1, 10) (2.8 MB)\n","\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0', 'onnxslim==0.1.34', 'onnxruntime'] not found, attempting AutoUpdate...\n","Collecting onnx>=1.12.0\n","  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Collecting onnxslim==0.1.34\n","  Downloading onnxslim-0.1.34-py3-none-any.whl.metadata (2.7 kB)\n","Collecting onnxruntime\n","  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxslim==0.1.34) (1.13.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxslim==0.1.34) (24.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (1.26.4)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (3.20.3)\n","Collecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxslim==0.1.34) (1.3.0)\n","Downloading onnxslim-0.1.34-py3-none-any.whl (140 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.3/140.3 kB 5.1 MB/s eta 0:00:00\n","Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.9/15.9 MB 205.3 MB/s eta 0:00:00\n","Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.2/13.2 MB 199.8 MB/s eta 0:00:00\n","Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 174.0 MB/s eta 0:00:00\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 221.6 MB/s eta 0:00:00\n","Installing collected packages: onnx, humanfriendly, onnxslim, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.16.2 onnxruntime-1.19.2 onnxslim-0.1.34\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 10.5s, installed 3 packages: ['onnx>=1.12.0', 'onnxslim==0.1.34', 'onnxruntime']\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.2 opset 12...\n","\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.34...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 11.0s, saved as '/content/runs/classify/train/weights/best.onnx' (5.5 MB)\n","\n","Export complete (11.2s)\n","Results saved to \u001b[1m/content/runs/classify/train/weights\u001b[0m\n","Predict:         yolo predict task=classify model=/content/runs/classify/train/weights/best.onnx imgsz=32  \n","Validate:        yolo val task=classify model=/content/runs/classify/train/weights/best.onnx imgsz=32 data=/content/dataset  \n","Visualize:       https://netron.app\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/runs/classify/train/weights/best.onnx'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["import onnxruntime as ort\n","from PIL import Image\n","import numpy as np\n","import os\n","\n","# Ruta del modelo ONNX exportado\n","onnx_model_path = r'/content/runs/classify/train/weights/best.onnx'\n","\n","# Cargar el modelo ONNX usando onnxruntime\n","session = ort.InferenceSession(onnx_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/cauto_test.jpg'  # Cambia esto a la ruta real de tu imagen\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Redimensionar la imagen al tamaño adecuado para CIFAR-10 (32x32)\n","image = image.resize((32, 32))\n","\n","# Convertir la imagen a un array numpy y escalar los valores de píxeles\n","image_np = np.array(image).astype(np.float32) / 255.0\n","\n","# Cambiar el formato de la imagen a (1, 3, 32, 32) -> (batch_size, canales, alto, ancho)\n","image_np = np.transpose(image_np, (2, 0, 1))  # Cambiar los ejes para que esté en formato canales primero\n","image_np = np.expand_dims(image_np, axis=0)   # Añadir la dimensión de batch_size\n","\n","# Realizar la predicción con el modelo ONNX\n","input_name = session.get_inputs()[0].name  # Obtener el nombre del primer input del modelo\n","output_name = session.get_outputs()[0].name  # Obtener el nombre del primer output del modelo\n","results = session.run([output_name], {input_name: image_np})\n","\n","# Procesar los resultados de la predicción\n","predicciones = results[0][0]  # Acceder a las predicciones de la primera imagen\n","clase_detectada = np.argmax(predicciones)  # Obtener la clase con mayor probabilidad\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOkueZsfwL9l","executionInfo":{"status":"ok","timestamp":1727736748337,"user_tz":360,"elapsed":368,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"602a4830-5d62-43f3-e533-7ab04437fefa"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Primera opción detectada: automobile con probabilidad 0.48\n","airplane: 0.1954\n","automobile: 0.4841\n","bird: 0.0154\n","cat: 0.0129\n","deer: 0.0179\n","dog: 0.0037\n","frog: 0.0310\n","horse: 0.0041\n","ship: 0.0523\n","truck: 0.1832\n"]}]},{"cell_type":"code","source":["import onnxruntime as ort\n","from PIL import Image\n","import numpy as np\n","import os\n","\n","# Ruta del modelo ONNX exportado\n","onnx_model_path = r'/content/runs/classify/train/weights/best.onnx'\n","\n","# Cargar el modelo ONNX usando onnxruntime\n","session = ort.InferenceSession(onnx_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/rana.jpg'  # Cambia esto a la ruta real de tu imagen\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Redimensionar la imagen al tamaño adecuado para CIFAR-10 (32x32)\n","image = image.resize((32, 32))\n","\n","# Convertir la imagen a un array numpy y escalar los valores de píxeles\n","image_np = np.array(image).astype(np.float32) / 255.0\n","\n","# Cambiar el formato de la imagen a (1, 3, 32, 32) -> (batch_size, canales, alto, ancho)\n","image_np = np.transpose(image_np, (2, 0, 1))  # Cambiar los ejes para que esté en formato canales primero\n","image_np = np.expand_dims(image_np, axis=0)   # Añadir la dimensión de batch_size\n","\n","# Realizar la predicción con el modelo ONNX\n","input_name = session.get_inputs()[0].name  # Obtener el nombre del primer input del modelo\n","output_name = session.get_outputs()[0].name  # Obtener el nombre del primer output del modelo\n","results = session.run([output_name], {input_name: image_np})\n","\n","# Procesar los resultados de la predicción\n","predicciones = results[0][0]  # Acceder a las predicciones de la primera imagen\n","clase_detectada = np.argmax(predicciones)  # Obtener la clase con mayor probabilidad\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sPfyBSkKwhUe","executionInfo":{"status":"ok","timestamp":1727736840802,"user_tz":360,"elapsed":394,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"b14980de-92b5-409e-edc5-a2a417a96404"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Primera opción detectada: frog con probabilidad 0.62\n","airplane: 0.0104\n","automobile: 0.0034\n","bird: 0.2915\n","cat: 0.0205\n","deer: 0.0387\n","dog: 0.0075\n","frog: 0.6178\n","horse: 0.0062\n","ship: 0.0007\n","truck: 0.0034\n"]}]},{"cell_type":"code","source":["import onnxruntime as ort\n","from PIL import Image\n","import numpy as np\n","import os\n","\n","# Ruta del modelo ONNX exportado\n","onnx_model_path = r'/content/runs/classify/train/weights/best.onnx'\n","\n","# Cargar el modelo ONNX usando onnxruntime\n","session = ort.InferenceSession(onnx_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Pelusa.jpeg'  # Cambia esto a la ruta real de tu imagen\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Redimensionar la imagen al tamaño adecuado para CIFAR-10 (32x32)\n","image = image.resize((32, 32))\n","\n","# Convertir la imagen a un array numpy y escalar los valores de píxeles\n","image_np = np.array(image).astype(np.float32) / 255.0\n","\n","# Cambiar el formato de la imagen a (1, 3, 32, 32) -> (batch_size, canales, alto, ancho)\n","image_np = np.transpose(image_np, (2, 0, 1))  # Cambiar los ejes para que esté en formato canales primero\n","image_np = np.expand_dims(image_np, axis=0)   # Añadir la dimensión de batch_size\n","\n","# Realizar la predicción con el modelo ONNX\n","input_name = session.get_inputs()[0].name  # Obtener el nombre del primer input del modelo\n","output_name = session.get_outputs()[0].name  # Obtener el nombre del primer output del modelo\n","results = session.run([output_name], {input_name: image_np})\n","\n","# Procesar los resultados de la predicción\n","predicciones = results[0][0]  # Acceder a las predicciones de la primera imagen\n","clase_detectada = np.argmax(predicciones)  # Obtener la clase con mayor probabilidad\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WacwJLW1wjwp","executionInfo":{"status":"ok","timestamp":1727736880875,"user_tz":360,"elapsed":216,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"96246f4b-f04e-4b8e-d77c-054e9a60f465"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Primera opción detectada: horse con probabilidad 0.30\n","airplane: 0.1914\n","automobile: 0.0304\n","bird: 0.0241\n","cat: 0.1396\n","deer: 0.0791\n","dog: 0.0640\n","frog: 0.0028\n","horse: 0.3022\n","ship: 0.0166\n","truck: 0.1498\n"]}]},{"cell_type":"code","source":["import onnxruntime as ort\n","from PIL import Image\n","import numpy as np\n","import os\n","\n","# Ruta del modelo ONNX exportado\n","onnx_model_path = r'/content/runs/classify/train/weights/best.onnx'\n","\n","# Cargar el modelo ONNX usando onnxruntime\n","session = ort.InferenceSession(onnx_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Pelusa_zoom.png'  # Cambia esto a la ruta real de tu imagen\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Redimensionar la imagen al tamaño adecuado para CIFAR-10 (32x32)\n","image = image.resize((32, 32))\n","\n","# Convertir la imagen a un array numpy y escalar los valores de píxeles\n","image_np = np.array(image).astype(np.float32) / 255.0\n","\n","# Cambiar el formato de la imagen a (1, 3, 32, 32) -> (batch_size, canales, alto, ancho)\n","image_np = np.transpose(image_np, (2, 0, 1))  # Cambiar los ejes para que esté en formato canales primero\n","image_np = np.expand_dims(image_np, axis=0)   # Añadir la dimensión de batch_size\n","\n","# Realizar la predicción con el modelo ONNX\n","input_name = session.get_inputs()[0].name  # Obtener el nombre del primer input del modelo\n","output_name = session.get_outputs()[0].name  # Obtener el nombre del primer output del modelo\n","results = session.run([output_name], {input_name: image_np})\n","\n","# Procesar los resultados de la predicción\n","predicciones = results[0][0]  # Acceder a las predicciones de la primera imagen\n","clase_detectada = np.argmax(predicciones)  # Obtener la clase con mayor probabilidad\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JEEUOv-zwnA0","executionInfo":{"status":"ok","timestamp":1727736924248,"user_tz":360,"elapsed":171,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"f6a56b2f-142f-4805-b8fe-31ffb0036e54"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Primera opción detectada: cat con probabilidad 0.44\n","airplane: 0.0071\n","automobile: 0.0221\n","bird: 0.0277\n","cat: 0.4395\n","deer: 0.0101\n","dog: 0.0287\n","frog: 0.1844\n","horse: 0.0172\n","ship: 0.1958\n","truck: 0.0675\n"]}]},{"cell_type":"code","source":["import onnxruntime as ort\n","from PIL import Image\n","import numpy as np\n","import os\n","\n","# Ruta del modelo ONNX exportado\n","onnx_model_path = r'/content/runs/classify/train/weights/best.onnx'\n","\n","# Cargar el modelo ONNX usando onnxruntime\n","session = ort.InferenceSession(onnx_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Minicky.jpeg'  # Cambia esto a la ruta real de tu imagen\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Redimensionar la imagen al tamaño adecuado para CIFAR-10 (32x32)\n","image = image.resize((32, 32))\n","\n","# Convertir la imagen a un array numpy y escalar los valores de píxeles\n","image_np = np.array(image).astype(np.float32) / 255.0\n","\n","# Cambiar el formato de la imagen a (1, 3, 32, 32) -> (batch_size, canales, alto, ancho)\n","image_np = np.transpose(image_np, (2, 0, 1))  # Cambiar los ejes para que esté en formato canales primero\n","image_np = np.expand_dims(image_np, axis=0)   # Añadir la dimensión de batch_size\n","\n","# Realizar la predicción con el modelo ONNX\n","input_name = session.get_inputs()[0].name  # Obtener el nombre del primer input del modelo\n","output_name = session.get_outputs()[0].name  # Obtener el nombre del primer output del modelo\n","results = session.run([output_name], {input_name: image_np})\n","\n","# Procesar los resultados de la predicción\n","predicciones = results[0][0]  # Acceder a las predicciones de la primera imagen\n","clase_detectada = np.argmax(predicciones)  # Obtener la clase con mayor probabilidad\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NeoHLP3owrMr","executionInfo":{"status":"ok","timestamp":1727736979938,"user_tz":360,"elapsed":376,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"58fcac87-dd2f-464c-d45e-178ec0e2f858"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Primera opción detectada: frog con probabilidad 0.28\n","airplane: 0.0874\n","automobile: 0.0174\n","bird: 0.1458\n","cat: 0.1666\n","deer: 0.0172\n","dog: 0.0085\n","frog: 0.2850\n","horse: 0.0043\n","ship: 0.2647\n","truck: 0.0029\n"]}]},{"cell_type":"code","source":["import onnxruntime as ort\n","from PIL import Image\n","import numpy as np\n","import os\n","\n","# Ruta del modelo ONNX exportado\n","onnx_model_path = r'/content/runs/classify/train/weights/best.onnx'\n","\n","# Cargar el modelo ONNX usando onnxruntime\n","session = ort.InferenceSession(onnx_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/gato3.JPG'  # Cambia esto a la ruta real de tu imagen\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Redimensionar la imagen al tamaño adecuado para CIFAR-10 (32x32)\n","image = image.resize((32, 32))\n","\n","# Convertir la imagen a un array numpy y escalar los valores de píxeles\n","image_np = np.array(image).astype(np.float32) / 255.0\n","\n","# Cambiar el formato de la imagen a (1, 3, 32, 32) -> (batch_size, canales, alto, ancho)\n","image_np = np.transpose(image_np, (2, 0, 1))  # Cambiar los ejes para que esté en formato canales primero\n","image_np = np.expand_dims(image_np, axis=0)   # Añadir la dimensión de batch_size\n","\n","# Realizar la predicción con el modelo ONNX\n","input_name = session.get_inputs()[0].name  # Obtener el nombre del primer input del modelo\n","output_name = session.get_outputs()[0].name  # Obtener el nombre del primer output del modelo\n","results = session.run([output_name], {input_name: image_np})\n","\n","# Procesar los resultados de la predicción\n","predicciones = results[0][0]  # Acceder a las predicciones de la primera imagen\n","clase_detectada = np.argmax(predicciones)  # Obtener la clase con mayor probabilidad\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ViVKQzOJv08","executionInfo":{"status":"ok","timestamp":1727737047313,"user_tz":360,"elapsed":342,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"d56f706d-270f-46b0-cfff-fccc60417f47"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Primera opción detectada: cat con probabilidad 0.40\n","airplane: 0.0318\n","automobile: 0.0027\n","bird: 0.1220\n","cat: 0.4019\n","deer: 0.0701\n","dog: 0.1703\n","frog: 0.0168\n","horse: 0.0888\n","ship: 0.0424\n","truck: 0.0532\n"]}]},{"cell_type":"code","source":["# Crear modelo en TorchScript\n","\n","model = YOLO(r'/content/runs/classify/train/weights/best.pt')  # load a custom trained model\n","\n","# Export the model\n","model.export(format='torchscript', opset=12, simplify=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"rgYJlQunwr8J","executionInfo":{"status":"ok","timestamp":1727737097033,"user_tz":360,"elapsed":2365,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"bd25c556-eeb2-4456-885c-c565d4f7e811"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.1 🚀 Python-3.10.12 torch-2.4.1+cu121 CPU (Intel Xeon 2.20GHz)\n","YOLOv8n-cls summary (fused): 73 layers, 1,447,690 parameters, 0 gradients, 3.3 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/runs/classify/train/weights/best.pt' with input shape (1, 3, 32, 32) BCHW and output shape(s) (1, 10) (2.8 MB)\n","\n","\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.4.1+cu121...\n","\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ✅ 1.7s, saved as '/content/runs/classify/train/weights/best.torchscript' (5.7 MB)\n","\n","Export complete (2.0s)\n","Results saved to \u001b[1m/content/runs/classify/train/weights\u001b[0m\n","Predict:         yolo predict task=classify model=/content/runs/classify/train/weights/best.torchscript imgsz=32  \n","Validate:        yolo val task=classify model=/content/runs/classify/train/weights/best.torchscript imgsz=32 data=/content/dataset  \n","Visualize:       https://netron.app\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/runs/classify/train/weights/best.torchscript'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Minicky.jpeg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpwwbjFnwuMT","executionInfo":{"status":"ok","timestamp":1727737130127,"user_tz":360,"elapsed":688,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"eab3b50f-10ff-44b8-f386-a13236e61426"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [    0.10504    0.013874    0.027439     0.56024    0.041749     0.20344    0.034528   0.0090284   0.0033908   0.0012658]\n","Primera opción detectada: cat con probabilidad 0.56\n","airplane: 0.1050\n","automobile: 0.0139\n","bird: 0.0274\n","cat: 0.5602\n","deer: 0.0417\n","dog: 0.2034\n","frog: 0.0345\n","horse: 0.0090\n","ship: 0.0034\n","truck: 0.0013\n"]}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/rana.jpg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6U_n1pew0PG","executionInfo":{"status":"ok","timestamp":1727737409361,"user_tz":360,"elapsed":585,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"51559b1b-1402-4e71-fafd-59d3a61b227c"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [    0.77726   0.0049556     0.01801     0.12602   0.0024344    0.055951   0.0011027   0.0087157    0.001655   0.0038966]\n","Primera opción detectada: airplane con probabilidad 0.78\n","airplane: 0.7773\n","automobile: 0.0050\n","bird: 0.0180\n","cat: 0.1260\n","deer: 0.0024\n","dog: 0.0560\n","frog: 0.0011\n","horse: 0.0087\n","ship: 0.0017\n","truck: 0.0039\n"]}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/cauto_test.jpg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyHmh1onw27m","executionInfo":{"status":"ok","timestamp":1727737709312,"user_tz":360,"elapsed":382,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"c2f0ef79-4cfb-4511-f9bd-860c08bed4bc"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [    0.63165   0.0026282    0.045723     0.19616    0.006656    0.093664    0.013086   0.0048382   0.0046807  0.00091656]\n","Primera opción detectada: airplane con probabilidad 0.63\n","airplane: 0.6317\n","automobile: 0.0026\n","bird: 0.0457\n","cat: 0.1962\n","deer: 0.0067\n","dog: 0.0937\n","frog: 0.0131\n","horse: 0.0048\n","ship: 0.0047\n","truck: 0.0009\n"]}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Pelusa.jpeg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K2nacmrcw5SO","executionInfo":{"status":"ok","timestamp":1727737850656,"user_tz":360,"elapsed":635,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"388eeb1b-70e1-4041-eb6e-41ab4c25b878"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [    0.12877   0.0068826    0.028877     0.16636     0.01615     0.17551   0.0034019     0.45025    0.011753    0.012043]\n","Primera opción detectada: horse con probabilidad 0.45\n","airplane: 0.1288\n","automobile: 0.0069\n","bird: 0.0289\n","cat: 0.1664\n","deer: 0.0161\n","dog: 0.1755\n","frog: 0.0034\n","horse: 0.4502\n","ship: 0.0118\n","truck: 0.0120\n"]}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/cauto_test.jpg'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727737807084,"user_tz":360,"elapsed":463,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"158b3b4c-232b-4ffa-bdca-62b61833315b","id":"uYZzirrCMryv"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [    0.63165   0.0026282    0.045723     0.19616    0.006656    0.093664    0.013086   0.0048382   0.0046807  0.00091656]\n","Primera opción detectada: airplane con probabilidad 0.63\n","airplane: 0.6317\n","automobile: 0.0026\n","bird: 0.0457\n","cat: 0.1962\n","deer: 0.0067\n","dog: 0.0937\n","frog: 0.0131\n","horse: 0.0048\n","ship: 0.0047\n","truck: 0.0009\n"]}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/Pelusa_zoom.png'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwJYkTxZw7tM","executionInfo":{"status":"ok","timestamp":1727737976023,"user_tz":360,"elapsed":758,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"7652a5b5-7e04-4ac0-ad9e-4404989f59bf"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [    0.51649    0.003986    0.073094     0.29354    0.012327    0.065598    0.014952     0.01254    0.003231   0.0042442]\n","Primera opción detectada: airplane con probabilidad 0.52\n","airplane: 0.5165\n","automobile: 0.0040\n","bird: 0.0731\n","cat: 0.2935\n","deer: 0.0123\n","dog: 0.0656\n","frog: 0.0150\n","horse: 0.0125\n","ship: 0.0032\n","truck: 0.0042\n"]}]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","# Cargar el modelo TorchScript\n","torchscript_model_path = r'/content/runs/classify/train/weights/best.torchscript'\n","torchscript_model = torch.jit.load(torchscript_model_path)\n","\n","# Ruta de la imagen local\n","image_path = r'/content/gato3.JPG'\n","\n","# Cargar la imagen desde el sistema de archivos\n","image = Image.open(image_path)\n","\n","# Verificar si la imagen está en RGB, y convertir si es necesario\n","if image.mode != 'RGB':\n","    image = image.convert('RGB')\n","\n","# Preprocesar la imagen: redimensionar, convertir a tensor y normalizar\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Redimensionar al tamaño requerido (CIFAR-10 usa 32x32)\n","    transforms.ToTensor(),        # Convertir la imagen a un tensor\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización (esto depende de cómo se entrenó tu modelo)\n","])\n","\n","# Aplicar las transformaciones a la imagen\n","image_tensor = transform(image)\n","\n","# Añadir una dimensión adicional para representar el batch (batch_size=1)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# Pasar la imagen a través del modelo TorchScript\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para la inferencia\n","    results = torchscript_model(image_tensor)\n","\n","# Verificar la forma del tensor de predicciones\n","print(f\"Forma de las predicciones: {results.shape}\")\n","\n","# Acceder a las predicciones de la primera imagen (ajusta según la forma del tensor)\n","predicciones = results[0].numpy() if results.dim() > 1 else results.numpy()\n","\n","# Verificar si las predicciones tienen el formato correcto\n","print(f\"Predicciones (primera imagen): {predicciones}\")\n","\n","# Obtener la clase con mayor probabilidad\n","clase_detectada = np.argmax(predicciones)\n","probabilidad = np.max(predicciones)\n","\n","# Nombres de las clases del CIFAR-10\n","clases = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Mostrar la primera opción detectada\n","print(f\"Primera opción detectada: {clases[clase_detectada]} con probabilidad {probabilidad:.2f}\")\n","\n","# Si quieres ver todas las probabilidades\n","for i, prob in enumerate(predicciones):\n","    print(f\"{clases[i]}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1RVEs1i5w-Jf","executionInfo":{"status":"ok","timestamp":1727738026145,"user_tz":360,"elapsed":441,"user":{"displayName":"Luis Ángel :D","userId":"16368634927928023201"}},"outputId":"fd44fabe-1964-4e1d-c09c-67ad1e74c4e8"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma de las predicciones: torch.Size([1, 10])\n","Predicciones (primera imagen): [    0.32461    0.014862     0.31413     0.20247    0.015216    0.050771   0.0071838    0.021125    0.024379    0.025253]\n","Primera opción detectada: airplane con probabilidad 0.32\n","airplane: 0.3246\n","automobile: 0.0149\n","bird: 0.3141\n","cat: 0.2025\n","deer: 0.0152\n","dog: 0.0508\n","frog: 0.0072\n","horse: 0.0211\n","ship: 0.0244\n","truck: 0.0253\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPT7O0HiO+w7xzHc0hxmfjl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}